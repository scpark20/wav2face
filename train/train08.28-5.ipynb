{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79d0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6665403e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 30 02:43:02 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000    Off  | 00000000:1B:00.0 Off |                    0 |\n",
      "| 30%   24C    P8    13W / 230W |  13466MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A5000    Off  | 00000000:1C:00.0 Off |                  Off |\n",
      "| 30%   26C    P8    14W / 230W |    508MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A5000    Off  | 00000000:1D:00.0 Off |                  Off |\n",
      "| 30%   24C    P8    16W / 230W |   1082MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A5000    Off  | 00000000:1E:00.0 Off |                  Off |\n",
      "| 33%   65C    P2   229W / 230W |  21988MiB / 24564MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA RTX A5000    Off  | 00000000:89:00.0 Off |                  Off |\n",
      "| 36%   65C    P2   219W / 230W |  20952MiB / 24564MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA RTX A5000    Off  | 00000000:8A:00.0 Off |                  Off |\n",
      "| 33%   59C    P2   164W / 230W |  21988MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA RTX A5000    Off  | 00000000:8B:00.0 Off |                  Off |\n",
      "| 30%   30C    P8    18W / 230W |    804MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA RTX A5000    Off  | 00000000:8C:00.0 Off |                  Off |\n",
      "| 30%   23C    P8    17W / 230W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A   3490700      C   ...nda3/envs/ste2/bin/python     3118MiB |\n",
      "|    0   N/A  N/A   3551971      C   ...nda3/envs/ste2/bin/python     5178MiB |\n",
      "|    0   N/A  N/A   3599842      C   ...nda3/envs/ste2/bin/python     5162MiB |\n",
      "|    1   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   3551021      C   ...onda3/envs/ste/bin/python      500MiB |\n",
      "|    2   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A     36805      C   ...onda3/envs/ste/bin/python     1074MiB |\n",
      "|    3   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A   3022180      C   ...nda3/envs/ste2/bin/python    21980MiB |\n",
      "|    4   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    4   N/A  N/A   3061306      C   ...nda3/envs/ste2/bin/python    20944MiB |\n",
      "|    5   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    5   N/A  N/A    287500      C   ...nda3/envs/ste2/bin/python    21980MiB |\n",
      "|    6   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    6   N/A  N/A   3289182      C   ...nda3/envs/ste2/bin/python      796MiB |\n",
      "|    7   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "!nvidia-smi\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee05e12",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb821737",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 1024\n",
    "n_outputs = 61\n",
    "n_frames = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c54e68",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cedcf798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from model.model_ecapa_transformer_reg import Model\n",
    "from utils.util import *\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "step = 0\n",
    "device = 'cuda:0'\n",
    "\n",
    "# Model\n",
    "model = Model(in_dim=n_mels, h_dim=512, out_dim=n_outputs)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31748b8c",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998d7f8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n"
     ]
    }
   ],
   "source": [
    "save_dir = '/data/scpark/save/lips/train08.28-5/'\n",
    "!mkdir -p $save_dir\n",
    "!ls -lt $save_dir\n",
    "\n",
    "writer = SummaryWriter(save_dir)\n",
    "\n",
    "if False:\n",
    "    step, model, _, optimizer = load(save_dir, 7321, model, None, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2f5fde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events.out.tfevents.1693214128.GPUSVR01  save_0      save_20000\r\n",
      "events.out.tfevents.1693236587.GPUSVR01  save_10000  save_30000\r\n"
     ]
    }
   ],
   "source": [
    "!ls /data/scpark/save/lips/train08.28-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2742e6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker_embedding.blocks.0.conv.conv.weight\n",
      "speaker_embedding.blocks.0.conv.conv.bias\n",
      "speaker_embedding.blocks.0.norm.norm.weight\n",
      "speaker_embedding.blocks.0.norm.norm.bias\n",
      "speaker_embedding.blocks.0.norm.norm.running_mean\n",
      "speaker_embedding.blocks.0.norm.norm.running_var\n",
      "speaker_embedding.blocks.0.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.tdnn1.conv.conv.weight\n",
      "speaker_embedding.blocks.1.tdnn1.conv.conv.bias\n",
      "speaker_embedding.blocks.1.tdnn1.norm.norm.weight\n",
      "speaker_embedding.blocks.1.tdnn1.norm.norm.bias\n",
      "speaker_embedding.blocks.1.tdnn1.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.tdnn1.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.tdnn1.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.0.conv.conv.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.0.conv.conv.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.0.norm.norm.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.0.norm.norm.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.0.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.0.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.0.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.1.conv.conv.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.1.conv.conv.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.1.norm.norm.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.1.norm.norm.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.1.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.1.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.1.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.2.conv.conv.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.2.conv.conv.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.2.norm.norm.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.2.norm.norm.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.2.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.2.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.2.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.3.conv.conv.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.3.conv.conv.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.3.norm.norm.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.3.norm.norm.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.3.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.3.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.3.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.4.conv.conv.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.4.conv.conv.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.4.norm.norm.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.4.norm.norm.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.4.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.4.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.4.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.5.conv.conv.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.5.conv.conv.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.5.norm.norm.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.5.norm.norm.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.5.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.5.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.5.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.6.conv.conv.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.6.conv.conv.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.6.norm.norm.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.6.norm.norm.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.6.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.6.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.6.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.tdnn2.conv.conv.weight\n",
      "speaker_embedding.blocks.1.tdnn2.conv.conv.bias\n",
      "speaker_embedding.blocks.1.tdnn2.norm.norm.weight\n",
      "speaker_embedding.blocks.1.tdnn2.norm.norm.bias\n",
      "speaker_embedding.blocks.1.tdnn2.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.tdnn2.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.tdnn2.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.se_block.conv1.conv.weight\n",
      "speaker_embedding.blocks.1.se_block.conv1.conv.bias\n",
      "speaker_embedding.blocks.1.se_block.conv2.conv.weight\n",
      "speaker_embedding.blocks.1.se_block.conv2.conv.bias\n",
      "speaker_embedding.blocks.2.tdnn1.conv.conv.weight\n",
      "speaker_embedding.blocks.2.tdnn1.conv.conv.bias\n",
      "speaker_embedding.blocks.2.tdnn1.norm.norm.weight\n",
      "speaker_embedding.blocks.2.tdnn1.norm.norm.bias\n",
      "speaker_embedding.blocks.2.tdnn1.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.tdnn1.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.tdnn1.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.0.conv.conv.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.0.conv.conv.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.0.norm.norm.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.0.norm.norm.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.0.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.0.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.0.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.1.conv.conv.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.1.conv.conv.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.1.norm.norm.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.1.norm.norm.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.1.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.1.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.1.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.2.conv.conv.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.2.conv.conv.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.2.norm.norm.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.2.norm.norm.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.2.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.2.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.2.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.3.conv.conv.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.3.conv.conv.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.3.norm.norm.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.3.norm.norm.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.3.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.3.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.3.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.4.conv.conv.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.4.conv.conv.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.4.norm.norm.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.4.norm.norm.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.4.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.4.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.4.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.5.conv.conv.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.5.conv.conv.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.5.norm.norm.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.5.norm.norm.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.5.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.5.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.5.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.6.conv.conv.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.6.conv.conv.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.6.norm.norm.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.6.norm.norm.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.6.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.6.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.6.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.tdnn2.conv.conv.weight\n",
      "speaker_embedding.blocks.2.tdnn2.conv.conv.bias\n",
      "speaker_embedding.blocks.2.tdnn2.norm.norm.weight\n",
      "speaker_embedding.blocks.2.tdnn2.norm.norm.bias\n",
      "speaker_embedding.blocks.2.tdnn2.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.tdnn2.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.tdnn2.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.se_block.conv1.conv.weight\n",
      "speaker_embedding.blocks.2.se_block.conv1.conv.bias\n",
      "speaker_embedding.blocks.2.se_block.conv2.conv.weight\n",
      "speaker_embedding.blocks.2.se_block.conv2.conv.bias\n",
      "speaker_embedding.blocks.3.tdnn1.conv.conv.weight\n",
      "speaker_embedding.blocks.3.tdnn1.conv.conv.bias\n",
      "speaker_embedding.blocks.3.tdnn1.norm.norm.weight\n",
      "speaker_embedding.blocks.3.tdnn1.norm.norm.bias\n",
      "speaker_embedding.blocks.3.tdnn1.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.tdnn1.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.tdnn1.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.0.conv.conv.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.0.conv.conv.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.0.norm.norm.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.0.norm.norm.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.0.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.0.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.0.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.1.conv.conv.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.1.conv.conv.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.1.norm.norm.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.1.norm.norm.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.1.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.1.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.1.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.2.conv.conv.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.2.conv.conv.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.2.norm.norm.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.2.norm.norm.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.2.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.2.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.2.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.3.conv.conv.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.3.conv.conv.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.3.norm.norm.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.3.norm.norm.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.3.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.3.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.3.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.4.conv.conv.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.4.conv.conv.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.4.norm.norm.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.4.norm.norm.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.4.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.4.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.4.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.5.conv.conv.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.5.conv.conv.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.5.norm.norm.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.5.norm.norm.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.5.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.5.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.5.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.6.conv.conv.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.6.conv.conv.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.6.norm.norm.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.6.norm.norm.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.6.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.6.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.6.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.tdnn2.conv.conv.weight\n",
      "speaker_embedding.blocks.3.tdnn2.conv.conv.bias\n",
      "speaker_embedding.blocks.3.tdnn2.norm.norm.weight\n",
      "speaker_embedding.blocks.3.tdnn2.norm.norm.bias\n",
      "speaker_embedding.blocks.3.tdnn2.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.tdnn2.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.tdnn2.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.se_block.conv1.conv.weight\n",
      "speaker_embedding.blocks.3.se_block.conv1.conv.bias\n",
      "speaker_embedding.blocks.3.se_block.conv2.conv.weight\n",
      "speaker_embedding.blocks.3.se_block.conv2.conv.bias\n",
      "speaker_embedding.mfa.conv.conv.weight\n",
      "speaker_embedding.mfa.conv.conv.bias\n",
      "speaker_embedding.mfa.norm.norm.weight\n",
      "speaker_embedding.mfa.norm.norm.bias\n",
      "speaker_embedding.mfa.norm.norm.running_mean\n",
      "speaker_embedding.mfa.norm.norm.running_var\n",
      "speaker_embedding.mfa.norm.norm.num_batches_tracked\n",
      "speaker_embedding.asp.tdnn.conv.conv.weight\n",
      "speaker_embedding.asp.tdnn.conv.conv.bias\n",
      "speaker_embedding.asp.tdnn.norm.norm.weight\n",
      "speaker_embedding.asp.tdnn.norm.norm.bias\n",
      "speaker_embedding.asp.tdnn.norm.norm.running_mean\n",
      "speaker_embedding.asp.tdnn.norm.norm.running_var\n",
      "speaker_embedding.asp.tdnn.norm.norm.num_batches_tracked\n",
      "speaker_embedding.asp.conv.conv.weight\n",
      "speaker_embedding.asp.conv.conv.bias\n",
      "speaker_embedding.asp_bn.norm.weight\n",
      "speaker_embedding.asp_bn.norm.bias\n",
      "speaker_embedding.asp_bn.norm.running_mean\n",
      "speaker_embedding.asp_bn.norm.running_var\n",
      "speaker_embedding.asp_bn.norm.num_batches_tracked\n",
      "speaker_embedding.fc.conv.weight\n",
      "speaker_embedding.fc.conv.bias\n",
      "speaker_linear.weight\n",
      "speaker_linear.bias\n",
      "prenet.weight\n",
      "prenet.bias\n",
      "encoder.attn_layers.0.emb_rel_k\n",
      "encoder.attn_layers.0.emb_rel_v\n",
      "encoder.attn_layers.0.conv_q.weight\n",
      "encoder.attn_layers.0.conv_q.bias\n",
      "encoder.attn_layers.0.conv_k.weight\n",
      "encoder.attn_layers.0.conv_k.bias\n",
      "encoder.attn_layers.0.conv_v.weight\n",
      "encoder.attn_layers.0.conv_v.bias\n",
      "encoder.attn_layers.0.conv_o.weight\n",
      "encoder.attn_layers.0.conv_o.bias\n",
      "encoder.attn_layers.1.emb_rel_k\n",
      "encoder.attn_layers.1.emb_rel_v\n",
      "encoder.attn_layers.1.conv_q.weight\n",
      "encoder.attn_layers.1.conv_q.bias\n",
      "encoder.attn_layers.1.conv_k.weight\n",
      "encoder.attn_layers.1.conv_k.bias\n",
      "encoder.attn_layers.1.conv_v.weight\n",
      "encoder.attn_layers.1.conv_v.bias\n",
      "encoder.attn_layers.1.conv_o.weight\n",
      "encoder.attn_layers.1.conv_o.bias\n",
      "encoder.attn_layers.2.emb_rel_k\n",
      "encoder.attn_layers.2.emb_rel_v\n",
      "encoder.attn_layers.2.conv_q.weight\n",
      "encoder.attn_layers.2.conv_q.bias\n",
      "encoder.attn_layers.2.conv_k.weight\n",
      "encoder.attn_layers.2.conv_k.bias\n",
      "encoder.attn_layers.2.conv_v.weight\n",
      "encoder.attn_layers.2.conv_v.bias\n",
      "encoder.attn_layers.2.conv_o.weight\n",
      "encoder.attn_layers.2.conv_o.bias\n",
      "encoder.attn_layers.3.emb_rel_k\n",
      "encoder.attn_layers.3.emb_rel_v\n",
      "encoder.attn_layers.3.conv_q.weight\n",
      "encoder.attn_layers.3.conv_q.bias\n",
      "encoder.attn_layers.3.conv_k.weight\n",
      "encoder.attn_layers.3.conv_k.bias\n",
      "encoder.attn_layers.3.conv_v.weight\n",
      "encoder.attn_layers.3.conv_v.bias\n",
      "encoder.attn_layers.3.conv_o.weight\n",
      "encoder.attn_layers.3.conv_o.bias\n",
      "encoder.attn_layers.4.emb_rel_k\n",
      "encoder.attn_layers.4.emb_rel_v\n",
      "encoder.attn_layers.4.conv_q.weight\n",
      "encoder.attn_layers.4.conv_q.bias\n",
      "encoder.attn_layers.4.conv_k.weight\n",
      "encoder.attn_layers.4.conv_k.bias\n",
      "encoder.attn_layers.4.conv_v.weight\n",
      "encoder.attn_layers.4.conv_v.bias\n",
      "encoder.attn_layers.4.conv_o.weight\n",
      "encoder.attn_layers.4.conv_o.bias\n",
      "encoder.attn_layers.5.emb_rel_k\n",
      "encoder.attn_layers.5.emb_rel_v\n",
      "encoder.attn_layers.5.conv_q.weight\n",
      "encoder.attn_layers.5.conv_q.bias\n",
      "encoder.attn_layers.5.conv_k.weight\n",
      "encoder.attn_layers.5.conv_k.bias\n",
      "encoder.attn_layers.5.conv_v.weight\n",
      "encoder.attn_layers.5.conv_v.bias\n",
      "encoder.attn_layers.5.conv_o.weight\n",
      "encoder.attn_layers.5.conv_o.bias\n",
      "encoder.norm_layers_1.0.gamma\n",
      "encoder.norm_layers_1.0.beta\n",
      "encoder.norm_layers_1.1.gamma\n",
      "encoder.norm_layers_1.1.beta\n",
      "encoder.norm_layers_1.2.gamma\n",
      "encoder.norm_layers_1.2.beta\n",
      "encoder.norm_layers_1.3.gamma\n",
      "encoder.norm_layers_1.3.beta\n",
      "encoder.norm_layers_1.4.gamma\n",
      "encoder.norm_layers_1.4.beta\n",
      "encoder.norm_layers_1.5.gamma\n",
      "encoder.norm_layers_1.5.beta\n",
      "encoder.ffn_layers.0.conv_1.weight\n",
      "encoder.ffn_layers.0.conv_1.bias\n",
      "encoder.ffn_layers.0.conv_2.weight\n",
      "encoder.ffn_layers.0.conv_2.bias\n",
      "encoder.ffn_layers.1.conv_1.weight\n",
      "encoder.ffn_layers.1.conv_1.bias\n",
      "encoder.ffn_layers.1.conv_2.weight\n",
      "encoder.ffn_layers.1.conv_2.bias\n",
      "encoder.ffn_layers.2.conv_1.weight\n",
      "encoder.ffn_layers.2.conv_1.bias\n",
      "encoder.ffn_layers.2.conv_2.weight\n",
      "encoder.ffn_layers.2.conv_2.bias\n",
      "encoder.ffn_layers.3.conv_1.weight\n",
      "encoder.ffn_layers.3.conv_1.bias\n",
      "encoder.ffn_layers.3.conv_2.weight\n",
      "encoder.ffn_layers.3.conv_2.bias\n",
      "encoder.ffn_layers.4.conv_1.weight\n",
      "encoder.ffn_layers.4.conv_1.bias\n",
      "encoder.ffn_layers.4.conv_2.weight\n",
      "encoder.ffn_layers.4.conv_2.bias\n",
      "encoder.ffn_layers.5.conv_1.weight\n",
      "encoder.ffn_layers.5.conv_1.bias\n",
      "encoder.ffn_layers.5.conv_2.weight\n",
      "encoder.ffn_layers.5.conv_2.bias\n",
      "encoder.norm_layers_2.0.gamma\n",
      "encoder.norm_layers_2.0.beta\n",
      "encoder.norm_layers_2.1.gamma\n",
      "encoder.norm_layers_2.1.beta\n",
      "encoder.norm_layers_2.2.gamma\n",
      "encoder.norm_layers_2.2.beta\n",
      "encoder.norm_layers_2.3.gamma\n",
      "encoder.norm_layers_2.3.beta\n",
      "encoder.norm_layers_2.4.gamma\n",
      "encoder.norm_layers_2.4.beta\n",
      "encoder.norm_layers_2.5.gamma\n",
      "encoder.norm_layers_2.5.beta\n",
      "postnet.weight\n",
      "postnet.bias\n",
      "warm start\n"
     ]
    }
   ],
   "source": [
    "# # warm start\n",
    "# checkpoint = torch.load('/data/scpark/save/lips/train08.28-1/save_30000', map_location=torch.device('cpu'))\n",
    "# model_state_dict = model.state_dict()\n",
    "\n",
    "# for key in checkpoint['model_state_dict']:\n",
    "#     if key in model_state_dict.keys():\n",
    "#         if checkpoint['model_state_dict'][key].shape == model_state_dict[key].shape:\n",
    "#             model_state_dict[key] = checkpoint['model_state_dict'][key]\n",
    "#             print(key)\n",
    "# model.load_state_dict(model_state_dict, strict=True)\n",
    "# print('warm start')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71d7b3c",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a8aa62c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_10_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_11_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_12_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_1_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_2_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_3_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_4_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_5_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_8_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_9_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_10_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_1_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_2_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_3_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_4_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_5_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_6_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_7_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_8_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_9_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_10_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_1_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_2_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_3_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_4_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_5_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_6_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_7_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_8_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_9_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_10_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_1_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_2_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_3_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_4_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_5_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_6_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_7_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_8_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_9_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_0_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_1_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_2_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_3_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_4_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_5_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_6_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_7_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_8_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_10_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_1_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_2_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_3_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_4_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_5_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_6_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_7_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_8_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_9_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_1_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_20_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_21_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_2_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_30_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_31_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_3_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_40_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_4_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_50_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_51_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_52_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_5_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_6_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_7_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_8_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_9_iPhone_raw.npy\n",
      "71 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from data.arkit_dataset import LipsDataset, CombinedDataset, CombinedCollate\n",
    "\n",
    "root_dir = '/data/speech/digital_human/preprocessed/'\n",
    "\n",
    "def get_files(dir):\n",
    "    data = []\n",
    "    files = sorted([os.path.join(dir, file) for file in os.listdir(dir)])\n",
    "    for file in files:\n",
    "        if file.endswith('.npy') and 'ARKit' in file:\n",
    "            data.append(file)\n",
    "        if os.path.isdir(file):\n",
    "            data.extend(get_files(os.path.join(dir, file)))\n",
    "    return data\n",
    "\n",
    "files = get_files(root_dir)\n",
    "print(len(files))\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "def get_sid(file):\n",
    "    names = ['No Speaker', 'jeewonPark', 'jinwooOh', 'kyuchulLee', 'kyuseokKim', 'nohsikPark', 'soochulPark', 'yehunHwang']\n",
    "    for sid, name in enumerate(names):\n",
    "        if name in file:\n",
    "            return sid\n",
    "    return 0\n",
    "\n",
    "for file in files:\n",
    "    sid = get_sid(file)\n",
    "    print(sid, file)\n",
    "    dataset = LipsDataset(file, n_mels, n_frames, sid=sid, mel=False)\n",
    "    if '_10_' in file:\n",
    "        test_datasets.append(dataset)\n",
    "    else:\n",
    "        train_datasets.append(dataset)\n",
    "print(len(train_datasets), len(test_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bc7cac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(CombinedDataset(train_datasets), \n",
    "                                           num_workers=16, shuffle=True, batch_size=32, collate_fn=CombinedCollate())\n",
    "test_loader = torch.utils.data.DataLoader(CombinedDataset(test_datasets), \n",
    "                                          num_workers=10, shuffle=True, batch_size=10, collate_fn=CombinedCollate())\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a05691c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "wav2vec = Wav2Vec2ForCTC.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\").to(device)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c5a57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Resample\n",
    "resample = Resample(24000, 16000).to(device)\n",
    "\n",
    "def get_states(wav, size, layer=12):\n",
    "    # 24k to 16k\n",
    "    wav = resample(wav)\n",
    "    # Get probs.\n",
    "    with torch.no_grad():\n",
    "        states = wav2vec(wav, output_hidden_states=True).hidden_states[layer].transpose(1, 2)\n",
    "    # Resizing\n",
    "    states = F.interpolate(states, size=size, mode='linear').detach()\n",
    "    return states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03238ce2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0 0.025031717494130135\n",
      "test : 1 0.026723837479948997\n",
      "test : 2 0.021844299510121346\n",
      "test : 3 0.022482337430119514\n",
      "test : 4 0.02317694015800953\n",
      "test : 5 0.024305811151862144\n",
      "test : 6 0.027186283841729164\n",
      "test : 7 0.03272698447108269\n",
      "test : 8 0.030932454392313957\n",
      "test : 9 0.025418460369110107\n",
      "test_loss : 0.025982914492487907\n",
      "saved /data/scpark/save/lips/train08.28-5/save_0\n",
      "1\n",
      "loss 0.019634883850812912\n",
      "1 0.019634883850812912\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "isnan = False\n",
    "while True:\n",
    "    if isnan:\n",
    "        break\n",
    "    for batch in train_loader:\n",
    "        targets = torch.Tensor(batch['blend']).transpose(1, 2).to(device)\n",
    "        inputs = get_states(torch.Tensor(batch['wav']).to(device), targets.shape[-1])\n",
    "        sid = torch.Tensor(batch['sid']).int().to(device)\n",
    "        \n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs, targets, sid)\n",
    "        \n",
    "        print(step)\n",
    "        loss = 0\n",
    "        for key in outputs.keys():\n",
    "            if 'loss' in key:\n",
    "                loss += outputs[key]\n",
    "                print(key, outputs[key].item())\n",
    "        if torch.isnan(loss):\n",
    "            isnan = True\n",
    "            break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(step, loss.item())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            writer.add_scalar('train_loss', loss.item(), step)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            display.clear_output()\n",
    "            \n",
    "            # (b, 2)\n",
    "            speaker = outputs['speaker'].data.cpu().numpy()\n",
    "            plt.figure(figsize=[8, 8])\n",
    "            plt.scatter(speaker[:, 0], speaker[:, 1])\n",
    "            plt.show()\n",
    "            \n",
    "            losses = []\n",
    "            for i, batch in enumerate(test_loader):\n",
    "                if i >= 10:\n",
    "                    break\n",
    "                    \n",
    "                targets = torch.Tensor(batch['blend']).transpose(1, 2).to(device)\n",
    "                inputs = get_states(torch.Tensor(batch['wav']).to(device), targets.shape[-1])\n",
    "                sid = torch.Tensor(batch['sid']).int().to(device)\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs, targets, sid)\n",
    "                    \n",
    "                loss = 0\n",
    "                for key in outputs.keys():\n",
    "                    if 'loss' in key:\n",
    "                        loss += outputs[key]\n",
    "                print('test :', i, loss.item())\n",
    "                losses.append(loss)        \n",
    "            \n",
    "            test_loss = torch.stack(losses).mean().item()\n",
    "            print('test_loss :', test_loss)\n",
    "            writer.add_scalar('test_loss', test_loss, step)\n",
    "            \n",
    "#             plt.figure(figsize=[18, 4])\n",
    "#             librosa.display.specshow(targets[0].data.cpu().numpy(), cmap='magma')\n",
    "#             plt.show()\n",
    "\n",
    "#             plt.figure(figsize=[18, 4])\n",
    "#             librosa.display.specshow(outputs['y_pred'][0].data.cpu().numpy(), cmap='magma')\n",
    "#             plt.show()\n",
    "            \n",
    "#             for i in [20, 37]:\n",
    "#                 plt.figure(figsize=[18, 2])\n",
    "#                 plt.title(str(i))\n",
    "#                 plt.plot(targets[0].data.cpu().numpy()[i])\n",
    "#                 plt.plot(outputs['y_pred'][0].data.cpu().numpy()[i])\n",
    "#                 plt.show()\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            save(save_dir, step, model, None, optimizer)\n",
    "    \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c85fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(save_dir, step, model, None, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35939c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (b, 2)\n",
    "speaker = outputs['speaker'].data.cpu().numpy()\n",
    "plt.figure(figsize=[8, 8])\n",
    "plt.scatter(speaker[:, 0], speaker[:, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08695381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2467159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e1031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
