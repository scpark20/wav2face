{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79d0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6665403e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 16 12:39:06 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  Off  | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   55C    P0   124W / 300W |  43522MiB / 80994MiB |      9%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80G...  Off  | 00000000:1C:00.0 Off |                    0 |\n",
      "| N/A   56C    P0    87W / 300W |  69599MiB / 80994MiB |     28%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80G...  Off  | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    68W / 300W |   2311MiB / 80994MiB |     11%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80G...  Off  | 00000000:1E:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    46W / 300W |     38MiB / 80994MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100 80G...  Off  | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    56W / 300W |     38MiB / 80994MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100 80G...  Off  | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    64W / 300W |   9107MiB / 80994MiB |     10%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100 80G...  Off  | 00000000:8B:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    65W / 300W |     38MiB / 80994MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100 80G...  Off  | 00000000:8C:00.0 Off |                    0 |\n",
      "| N/A   53C    P0   304W / 300W |  11427MiB / 80994MiB |     64%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2479      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    0   N/A  N/A    167404      C   ...envs/scpark/bin/python3.9     1559MiB |\n",
      "|    0   N/A  N/A   3493700      C   ...a3/envs/scpark/bin/python    41925MiB |\n",
      "|    1   N/A  N/A      2479      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    1   N/A  N/A   3450885      C   ...a3/envs/scpark/bin/python    69561MiB |\n",
      "|    2   N/A  N/A      2479      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    2   N/A  N/A   1055079      C   ...a3/envs/scpark/bin/python     2273MiB |\n",
      "|    3   N/A  N/A      2479      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    4   N/A  N/A      2479      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    5   N/A  N/A      2479      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    5   N/A  N/A    838590      C   ...a3/envs/scpark/bin/python     9069MiB |\n",
      "|    6   N/A  N/A      2479      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    7   N/A  N/A      2479      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    7   N/A  N/A    320885      C   ...a3/envs/scpark/bin/python    11389MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "!nvidia-smi\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee05e12",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb821737",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 80\n",
    "n_outputs = 61\n",
    "n_frames = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c54e68",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cedcf798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from model.model_conditional_transformer_reg import Model\n",
    "from utils.util import *\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "step = 0\n",
    "device = 'cuda:0'\n",
    "\n",
    "# Model\n",
    "model = Model(in_dim=n_mels, h_dim=512, out_dim=n_outputs)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31748b8c",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "998d7f8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 18693084\n",
      "-rw-rw-r-- 1 scpark scpark         0  8월 16 12:39 events.out.tfevents.1692157154.GPUSVR11\n",
      "-rw-rw-r-- 1 scpark scpark        40  8월 16 12:38 events.out.tfevents.1692016510.GPUSVR11\n",
      "-rw-rw-r-- 1 scpark scpark     10831  8월 16 12:19 events.out.tfevents.1692016522.GPUSVR11\n",
      "-rw-rw-r-- 1 scpark scpark 531711057  8월 16 11:57 save_130000\n",
      "-rw-rw-r-- 1 scpark scpark 531711057  8월 16 09:27 save_122627\n",
      "-rw-rw-r-- 1 scpark scpark 531711057  8월 16 08:30 save_120000\n",
      "-rw-rw-r-- 1 scpark scpark 531711057  8월 16 04:30 save_110000\n",
      "-rw-rw-r-- 1 scpark scpark 531711057  8월 16 00:39 save_100000\n",
      "-rw-rw-r-- 1 scpark scpark 531710603  8월 15 21:12 save_90000\n",
      "-rw-rw-r-- 1 scpark scpark 531710603  8월 15 17:36 save_80000\n",
      "-rw-rw-r-- 1 scpark scpark 531710603  8월 15 14:10 save_70000\n",
      "-rw-rw-r-- 1 scpark scpark 531710603  8월 15 10:45 save_60000\n",
      "-rw-rw-r-- 1 scpark scpark 531710603  8월 15 07:08 save_50000\n",
      "-rw-rw-r-- 1 scpark scpark 531710603  8월 15 03:33 save_40000\n",
      "-rw-rw-r-- 1 scpark scpark 531710603  8월 15 00:01 save_30000\n",
      "-rw-rw-r-- 1 scpark scpark        40  8월 14 21:34 events.out.tfevents.1691989855.GPUSVR11\n",
      "-rw-rw-r-- 1 scpark scpark      2376  8월 14 21:33 events.out.tfevents.1691989971.GPUSVR11\n",
      "-rw-rw-r-- 1 scpark scpark 531710219  8월 14 21:31 save_23000\n",
      "-rw-rw-r-- 1 scpark scpark 531710219  8월 14 21:11 save_22000\n",
      "-rw-rw-r-- 1 scpark scpark 531710219  8월 14 20:51 save_21000\n",
      "-rw-rw-r-- 1 scpark scpark 531710219  8월 14 20:30 save_20000\n",
      "-rw-rw-r-- 1 scpark scpark 531710219  8월 14 20:10 save_19000\n",
      "-rw-rw-r-- 1 scpark scpark 531710219  8월 14 19:50 save_18000\n",
      "-rw-rw-r-- 1 scpark scpark 531710219  8월 14 19:30 save_17000\n",
      "-rw-rw-r-- 1 scpark scpark 531710219  8월 14 19:11 save_16000\n",
      "-rw-rw-r-- 1 scpark scpark 531710219  8월 14 18:52 save_15000\n",
      "-rw-rw-r-- 1 scpark scpark 531710219  8월 14 18:32 save_14000\n",
      "-rw-rw-r-- 1 scpark scpark 531710219  8월 14 18:13 save_13000\n",
      "-rw-rw-r-- 1 scpark scpark 531710219  8월 14 17:53 save_12000\n",
      "-rw-rw-r-- 1 scpark scpark 531710219  8월 14 17:34 save_11000\n",
      "-rw-rw-r-- 1 scpark scpark 531710219  8월 14 17:14 save_10000\n",
      "-rw-rw-r-- 1 scpark scpark 531709765  8월 14 16:55 save_9000\n",
      "-rw-rw-r-- 1 scpark scpark 531709765  8월 14 16:35 save_8000\n",
      "-rw-rw-r-- 1 scpark scpark 531709765  8월 14 16:15 save_7000\n",
      "-rw-rw-r-- 1 scpark scpark 531709765  8월 14 15:56 save_6000\n",
      "-rw-rw-r-- 1 scpark scpark 531709765  8월 14 15:36 save_5000\n",
      "-rw-rw-r-- 1 scpark scpark 531709765  8월 14 15:16 save_4000\n",
      "-rw-rw-r-- 1 scpark scpark 531709765  8월 14 14:56 save_3000\n",
      "-rw-rw-r-- 1 scpark scpark 531709765  8월 14 14:39 save_2000\n",
      "-rw-rw-r-- 1 scpark scpark 531709765  8월 14 14:26 save_1000\n",
      "-rw-rw-r-- 1 scpark scpark 531702835  8월 14 14:14 save_0\n",
      "-rw-rw-r-- 1 scpark scpark       131  8월 14 13:52 events.out.tfevents.1691988481.GPUSVR11\n",
      "loaded /data/scpark/save/lips/train08.14-1/save_130000\n"
     ]
    }
   ],
   "source": [
    "save_dir = '/data/scpark/save/lips/train08.14-1/'\n",
    "!mkdir -p $save_dir\n",
    "!ls -lt $save_dir\n",
    "\n",
    "writer = SummaryWriter(save_dir)\n",
    "\n",
    "if True:\n",
    "    step, model, _, optimizer = load(save_dir, 130000, model, None, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71d7b3c",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a8aa62c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_10_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_11_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_12_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_1_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_2_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_3_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_4_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_5_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_8_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_9_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_10_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_1_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_2_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_3_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_4_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_5_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_6_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_7_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_8_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_9_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_10_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_1_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_2_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_3_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_4_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_5_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_6_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_7_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_8_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_9_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_10_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_1_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_2_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_3_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_4_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_5_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_6_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_7_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_8_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_9_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_0_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_1_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_2_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_3_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_4_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_5_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_6_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_7_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_8_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_10_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_1_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_2_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_3_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_4_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_5_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_6_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_7_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_8_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_9_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_1_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_20_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_21_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_2_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_30_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_31_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_3_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_40_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_4_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_50_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_51_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_52_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_5_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_6_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_7_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_8_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_9_iPhone_raw.npy\n",
      "71 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from data.arkit_dataset import LipsDataset, CombinedDataset, CombinedCollate\n",
    "\n",
    "root_dir = '/data/speech/digital_human/preprocessed/'\n",
    "\n",
    "def get_files(dir):\n",
    "    data = []\n",
    "    files = sorted([os.path.join(dir, file) for file in os.listdir(dir)])\n",
    "    for file in files:\n",
    "        if file.endswith('.npy') and 'ARKit' in file:\n",
    "            data.append(file)\n",
    "        if os.path.isdir(file):\n",
    "            data.extend(get_files(os.path.join(dir, file)))\n",
    "    return data\n",
    "\n",
    "files = get_files(root_dir)\n",
    "print(len(files))\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "def get_sid(file):\n",
    "    names = ['No Speaker', 'jeewonPark', 'jinwooOh', 'kyuchulLee', 'kyuseokKim', 'nohsikPark', 'soochulPark', 'yehunHwang']\n",
    "    for sid, name in enumerate(names):\n",
    "        if name in file:\n",
    "            return sid\n",
    "    return 0\n",
    "\n",
    "for file in files:\n",
    "    sid = get_sid(file)\n",
    "    print(sid, file)\n",
    "    dataset = LipsDataset(file, n_mels, n_frames, sid=sid, mel=False)\n",
    "    if '_10_' in file:\n",
    "        test_datasets.append(dataset)\n",
    "    else:\n",
    "        train_datasets.append(dataset)\n",
    "print(len(train_datasets), len(test_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bc7cac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(CombinedDataset(train_datasets), \n",
    "                                           num_workers=16, shuffle=True, batch_size=32, collate_fn=CombinedCollate())\n",
    "test_loader = torch.utils.data.DataLoader(CombinedDataset(test_datasets), \n",
    "                                          num_workers=10, shuffle=True, batch_size=10, collate_fn=CombinedCollate())\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a05691c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from data.audio import mel_spectrogram\n",
    "get_mel = partial(mel_spectrogram, n_fft=2048, num_mels=80, sampling_rate=24000, hop_size=800, win_size=2048, fmin=0, fmax=None, center=False, return_spec=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03238ce2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0 0.024187259376049042\n",
      "test : 1 0.02238691970705986\n",
      "test : 2 0.023663973435759544\n",
      "test : 3 0.021486671641469002\n",
      "test : 4 0.024181092157959938\n",
      "test : 5 0.0242387093603611\n",
      "test : 6 0.02429049462080002\n",
      "test : 7 0.026487665250897408\n",
      "test : 8 0.022167127579450607\n",
      "test : 9 0.023831920698285103\n",
      "test_loss : 0.023692181333899498\n",
      "saved /data/scpark/save/lips/train08.14-1/save_130000\n",
      "130001\n",
      "loss 0.005614820867776871\n",
      "130001 0.005614820867776871\n",
      "130002\n",
      "loss 0.005806388333439827\n",
      "130002 0.005806388333439827\n",
      "130003\n",
      "loss 0.005144009366631508\n",
      "130003 0.005144009366631508\n",
      "130004\n",
      "loss 0.005521457642316818\n",
      "130004 0.005521457642316818\n",
      "130005\n",
      "loss 0.00494764419272542\n",
      "130005 0.00494764419272542\n",
      "130006\n",
      "loss 0.005642285104840994\n",
      "130006 0.005642285104840994\n",
      "130007\n",
      "loss 0.005465252790600061\n",
      "130007 0.005465252790600061\n",
      "130008\n",
      "loss 0.005311181303113699\n",
      "130008 0.005311181303113699\n",
      "130009\n",
      "loss 0.0052077011205255985\n",
      "130009 0.0052077011205255985\n",
      "130010\n",
      "loss 0.005548741202801466\n",
      "130010 0.005548741202801466\n",
      "130011\n",
      "loss 0.005546980071812868\n",
      "130011 0.005546980071812868\n",
      "130012\n",
      "loss 0.005956199485808611\n",
      "130012 0.005956199485808611\n",
      "130013\n",
      "loss 0.005303619429469109\n",
      "130013 0.005303619429469109\n",
      "130014\n",
      "loss 0.005535058677196503\n",
      "130014 0.005535058677196503\n",
      "130015\n",
      "loss 0.00574488053098321\n",
      "130015 0.00574488053098321\n",
      "130016\n",
      "loss 0.006104355677962303\n",
      "130016 0.006104355677962303\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "isnan = False\n",
    "while True:\n",
    "    if isnan:\n",
    "        break\n",
    "    for batch in train_loader:\n",
    "        inputs = get_mel(torch.Tensor(batch['wav'])).to(device)\n",
    "        targets = torch.Tensor(batch['blend']).transpose(1, 2).to(device)\n",
    "        sid = torch.Tensor(batch['sid']).int().to(device)\n",
    "        \n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs, targets, sid)\n",
    "        \n",
    "        print(step)\n",
    "        loss = 0\n",
    "        for key in outputs.keys():\n",
    "            if 'loss' in key:\n",
    "                loss += outputs[key]\n",
    "                print(key, outputs[key].item())\n",
    "        if torch.isnan(loss):\n",
    "            isnan = True\n",
    "            break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(step, loss.item())\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            writer.add_scalar('train_loss', loss.item(), step)\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            display.clear_output()\n",
    "            \n",
    "            losses = []\n",
    "            for i, batch in enumerate(test_loader):\n",
    "                if i >= 10:\n",
    "                    break\n",
    "                    \n",
    "                inputs = get_mel(torch.Tensor(batch['wav'])).to(device)\n",
    "                targets = torch.Tensor(batch['blend']).transpose(1, 2).to(device)\n",
    "                sid = torch.Tensor(batch['sid']).int().to(device)\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs, targets, sid)\n",
    "                    \n",
    "                loss = 0\n",
    "                for key in outputs.keys():\n",
    "                    if 'loss' in key:\n",
    "                        loss += outputs[key]\n",
    "                print('test :', i, loss.item())\n",
    "                losses.append(loss)        \n",
    "            \n",
    "            test_loss = torch.stack(losses).mean().item()\n",
    "            print('test_loss :', test_loss)\n",
    "            writer.add_scalar('test_loss', test_loss, step)\n",
    "            \n",
    "#             plt.figure(figsize=[18, 4])\n",
    "#             librosa.display.specshow(targets[0].data.cpu().numpy(), cmap='magma')\n",
    "#             plt.show()\n",
    "\n",
    "#             plt.figure(figsize=[18, 4])\n",
    "#             librosa.display.specshow(outputs['y_pred'][0].data.cpu().numpy(), cmap='magma')\n",
    "#             plt.show()\n",
    "            \n",
    "#             for i in [20, 37]:\n",
    "#                 plt.figure(figsize=[18, 2])\n",
    "#                 plt.title(str(i))\n",
    "#                 plt.plot(targets[0].data.cpu().numpy()[i])\n",
    "#                 plt.plot(outputs['y_pred'][0].data.cpu().numpy()[i])\n",
    "#                 plt.show()\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            save(save_dir, step, model, None, optimizer)\n",
    "    \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c85fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(save_dir, step, model, None, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35939c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08695381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2467159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
