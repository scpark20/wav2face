{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79d0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6665403e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 14 13:14:07 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000    Off  | 00000000:1B:00.0 Off |                    0 |\n",
      "| 30%   27C    P8    13W / 230W |   1706MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A5000    Off  | 00000000:1C:00.0 Off |                  Off |\n",
      "| 30%   26C    P8    14W / 230W |   2866MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A5000    Off  | 00000000:1D:00.0 Off |                  Off |\n",
      "| 30%   26C    P8    15W / 230W |   2496MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A5000    Off  | 00000000:1E:00.0 Off |                  Off |\n",
      "| 30%   26C    P8    18W / 230W |   3122MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA RTX A5000    Off  | 00000000:89:00.0 Off |                  Off |\n",
      "| 30%   31C    P8    16W / 230W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA RTX A5000    Off  | 00000000:8A:00.0 Off |                  Off |\n",
      "| 30%   37C    P8    19W / 230W |   1320MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA RTX A5000    Off  | 00000000:8B:00.0 Off |                  Off |\n",
      "| 30%   56C    P2   217W / 230W |   9410MiB / 24564MiB |     84%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA RTX A5000    Off  | 00000000:8C:00.0 Off |                  Off |\n",
      "| 30%   26C    P8    16W / 230W |   3576MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3569      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A    393011      C   ...onda3/envs/ste/bin/python     1066MiB |\n",
      "|    0   N/A  N/A   1564894      C   ...onda3/envs/ste/bin/python      632MiB |\n",
      "|    1   N/A  N/A      3569      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A     10954      C   ...hyun/anaconda3/bin/python     2858MiB |\n",
      "|    2   N/A  N/A      3569      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A     10640      C   ...hyun/anaconda3/bin/python     2488MiB |\n",
      "|    3   N/A  N/A      3569      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A    218422      C   ...nda3/envs/byte/bin/python     3114MiB |\n",
      "|    4   N/A  N/A      3569      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    5   N/A  N/A      3569      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    5   N/A  N/A    245538      C   ...onda3/envs/ste/bin/python     1312MiB |\n",
      "|    6   N/A  N/A      3569      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    6   N/A  N/A   2240164      C   ...onda3/envs/ste/bin/python     9402MiB |\n",
      "|    7   N/A  N/A      3569      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    7   N/A  N/A    343993      C   ...3/envs/bytesep/bin/python     3568MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "!nvidia-smi\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee05e12",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb821737",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 80\n",
    "n_outputs = 61\n",
    "n_frames = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c54e68",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cedcf798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from model.model_transformer_reg import Model\n",
    "from utils.util import *\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "step = 0\n",
    "device = 'cuda:0'\n",
    "\n",
    "# Model\n",
    "model = Model(in_dim=n_mels, h_dim=1024, out_dim=n_outputs, n_layers=6, window_size=8)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18bd076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # warm start\n",
    "# checkpoint = torch.load('/data/scpark/save/lips/train06.23-1/save_120000', map_location=torch.device('cpu'))\n",
    "# model_state_dict = model.state_dict()\n",
    "\n",
    "# for key in checkpoint['model_state_dict']:\n",
    "#     if key in model_state_dict.keys():\n",
    "#         if checkpoint['model_state_dict'][key].shape == model_state_dict[key].shape:\n",
    "#             model_state_dict[key] = checkpoint['model_state_dict'][key]\n",
    "#             print(key)\n",
    "# model.load_state_dict(model_state_dict, strict=True)\n",
    "# print('warm start')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31748b8c",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "998d7f8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n"
     ]
    }
   ],
   "source": [
    "save_dir = '/data/scpark/save/lips/train07.14-1/'\n",
    "!mkdir -p $save_dir\n",
    "!ls -lt $save_dir\n",
    "\n",
    "writer = SummaryWriter(save_dir)\n",
    "\n",
    "if False:\n",
    "    step, model, _, optimizer = load(save_dir, 56389, model, None, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71d7b3c",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a8aa62c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_001_1_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_001_2_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_001_3_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_001_4_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_001_5_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_001_6_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_001_7_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_001_8_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_001_9_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_002_10_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_002_1_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_002_2_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_002_3_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_002_4_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_002_5_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_002_6_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_002_7_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_002_8_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_002_9_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_003_10_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_003_11_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_003_12_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_003_1_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_003_2_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_003_3_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_003_4_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_003_5_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_003_8_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_003_9_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_004_10_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_004_1_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_004_2_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_004_3_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_004_4_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_004_5_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_004_6_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_004_7_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_004_8_iPhone_raw.npy\n",
      "/data/speech/digital_human/preprocessed/MH_ARKit_004_9_iPhone_raw.npy\n",
      "36 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from data.arkit_dataset import LipsDataset, CombinedDataset, CombinedCollate\n",
    "\n",
    "root_dir = '/data/speech/digital_human/preprocessed/'\n",
    "files = sorted([os.path.join(root_dir, file) for file in os.listdir(root_dir)])\n",
    "print(len(files))\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "for file in files:\n",
    "    print(file)\n",
    "    dataset = LipsDataset(file, n_mels, n_frames)\n",
    "    if '_10_' in file:\n",
    "        test_datasets.append(dataset)\n",
    "    else:\n",
    "        train_datasets.append(dataset)\n",
    "print(len(train_datasets), len(test_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc7cac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(CombinedDataset(train_datasets), \n",
    "                                           num_workers=16, shuffle=True, batch_size=32, collate_fn=CombinedCollate())\n",
    "test_loader = torch.utils.data.DataLoader(CombinedDataset(test_datasets), \n",
    "                                          num_workers=10, shuffle=True, batch_size=10, collate_fn=CombinedCollate())\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03238ce2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0 1.5759835243225098\n",
      "test : 1 1.5748674869537354\n",
      "test : 2 1.5759663581848145\n",
      "test : 3 1.5770506858825684\n",
      "test : 4 1.5749685764312744\n",
      "test : 5 1.575554609298706\n",
      "test : 6 1.575116515159607\n",
      "test : 7 1.575774908065796\n",
      "test : 8 1.573891282081604\n",
      "test : 9 1.574268102645874\n",
      "test_loss : 1.575344204902649\n",
      "saved /data/scpark/save/lips/train07.14-1/save_0\n",
      "1\n",
      "loss 1.5433765649795532\n",
      "1 1.5433765649795532\n",
      "2\n",
      "loss 1.0331677198410034\n",
      "2 1.0331677198410034\n",
      "3\n",
      "loss 0.5754046440124512\n",
      "3 0.5754046440124512\n",
      "4\n",
      "loss 0.35595980286598206\n",
      "4 0.35595980286598206\n",
      "5\n",
      "loss 0.31548383831977844\n",
      "5 0.31548383831977844\n",
      "6\n",
      "loss 0.30122923851013184\n",
      "6 0.30122923851013184\n",
      "7\n",
      "loss 0.2689697742462158\n",
      "7 0.2689697742462158\n",
      "8\n",
      "loss 0.21715256571769714\n",
      "8 0.21715256571769714\n",
      "9\n",
      "loss 0.19307632744312286\n",
      "9 0.19307632744312286\n",
      "10\n",
      "loss 0.18477487564086914\n",
      "10 0.18477487564086914\n",
      "11\n",
      "loss 0.1798853576183319\n",
      "11 0.1798853576183319\n",
      "12\n",
      "loss 0.1693449467420578\n",
      "12 0.1693449467420578\n",
      "13\n",
      "loss 0.1568654179573059\n",
      "13 0.1568654179573059\n",
      "14\n",
      "loss 0.1543741226196289\n",
      "14 0.1543741226196289\n",
      "15\n",
      "loss 0.15291373431682587\n",
      "15 0.15291373431682587\n",
      "16\n",
      "loss 0.15250258147716522\n",
      "16 0.15250258147716522\n",
      "17\n",
      "loss 0.14707382023334503\n",
      "17 0.14707382023334503\n",
      "18\n",
      "loss 0.14145343005657196\n",
      "18 0.14145343005657196\n",
      "19\n",
      "loss 0.13898837566375732\n",
      "19 0.13898837566375732\n",
      "20\n",
      "loss 0.13842235505580902\n",
      "20 0.13842235505580902\n",
      "21\n",
      "loss 0.1381205916404724\n",
      "21 0.1381205916404724\n",
      "22\n",
      "loss 0.1338418871164322\n",
      "22 0.1338418871164322\n",
      "23\n",
      "loss 0.1336517184972763\n",
      "23 0.1336517184972763\n",
      "24\n",
      "loss 0.12950138747692108\n",
      "24 0.12950138747692108\n",
      "25\n",
      "loss 0.13367651402950287\n",
      "25 0.13367651402950287\n",
      "26\n",
      "loss 0.12719210982322693\n",
      "26 0.12719210982322693\n",
      "27\n",
      "loss 0.12812243402004242\n",
      "27 0.12812243402004242\n",
      "28\n",
      "loss 0.12737496197223663\n",
      "28 0.12737496197223663\n",
      "29\n",
      "loss 0.12503276765346527\n",
      "29 0.12503276765346527\n",
      "30\n",
      "loss 0.12374193221330643\n",
      "30 0.12374193221330643\n",
      "31\n",
      "loss 0.12497171014547348\n",
      "31 0.12497171014547348\n",
      "32\n",
      "loss 0.12260051816701889\n",
      "32 0.12260051816701889\n",
      "33\n",
      "loss 0.12043023109436035\n",
      "33 0.12043023109436035\n",
      "34\n",
      "loss 0.11922989785671234\n",
      "34 0.11922989785671234\n",
      "35\n",
      "loss 0.12081226706504822\n",
      "35 0.12081226706504822\n",
      "36\n",
      "loss 0.1204838976264\n",
      "36 0.1204838976264\n",
      "37\n",
      "loss 0.11561178416013718\n",
      "37 0.11561178416013718\n",
      "38\n",
      "loss 0.11556954681873322\n",
      "38 0.11556954681873322\n",
      "39\n",
      "loss 0.11516150087118149\n",
      "39 0.11516150087118149\n",
      "40\n",
      "loss 0.11664020270109177\n",
      "40 0.11664020270109177\n",
      "41\n",
      "loss 0.11642540991306305\n",
      "41 0.11642540991306305\n",
      "42\n",
      "loss 0.11406552046537399\n",
      "42 0.11406552046537399\n",
      "43\n",
      "loss 0.11347585916519165\n",
      "43 0.11347585916519165\n",
      "44\n",
      "loss 0.11129734665155411\n",
      "44 0.11129734665155411\n",
      "45\n",
      "loss 0.11209961026906967\n",
      "45 0.11209961026906967\n",
      "46\n",
      "loss 0.11258172988891602\n",
      "46 0.11258172988891602\n",
      "47\n",
      "loss 0.10897281020879745\n",
      "47 0.10897281020879745\n",
      "48\n",
      "loss 0.10867177695035934\n",
      "48 0.10867177695035934\n",
      "49\n",
      "loss 0.10732994228601456\n",
      "49 0.10732994228601456\n",
      "50\n",
      "loss 0.11038674414157867\n",
      "50 0.11038674414157867\n",
      "51\n",
      "loss 0.10856055468320847\n",
      "51 0.10856055468320847\n",
      "52\n",
      "loss 0.1085202768445015\n",
      "52 0.1085202768445015\n",
      "53\n",
      "loss 0.10711652040481567\n",
      "53 0.10711652040481567\n",
      "54\n",
      "loss 0.11011175811290741\n",
      "54 0.11011175811290741\n",
      "55\n",
      "loss 0.10769122838973999\n",
      "55 0.10769122838973999\n",
      "56\n",
      "loss 0.10688448697328568\n",
      "56 0.10688448697328568\n",
      "57\n",
      "loss 0.10739585757255554\n",
      "57 0.10739585757255554\n",
      "58\n",
      "loss 0.1044895276427269\n",
      "58 0.1044895276427269\n",
      "59\n",
      "loss 0.10442857444286346\n",
      "59 0.10442857444286346\n",
      "60\n",
      "loss 0.10620641708374023\n",
      "60 0.10620641708374023\n",
      "61\n",
      "loss 0.10624002665281296\n",
      "61 0.10624002665281296\n",
      "62\n",
      "loss 0.10641101002693176\n",
      "62 0.10641101002693176\n",
      "63\n",
      "loss 0.10694540292024612\n",
      "63 0.10694540292024612\n",
      "64\n",
      "loss 0.10406403988599777\n",
      "64 0.10406403988599777\n",
      "65\n",
      "loss 0.10467752069234848\n",
      "65 0.10467752069234848\n",
      "66\n",
      "loss 0.10547266900539398\n",
      "66 0.10547266900539398\n",
      "67\n",
      "loss 0.10466452687978745\n",
      "67 0.10466452687978745\n",
      "68\n",
      "loss 0.10422646254301071\n",
      "68 0.10422646254301071\n",
      "69\n",
      "loss 0.10505638271570206\n",
      "69 0.10505638271570206\n",
      "70\n",
      "loss 0.1052926778793335\n",
      "70 0.1052926778793335\n",
      "71\n",
      "loss 0.1036064550280571\n",
      "71 0.1036064550280571\n",
      "72\n",
      "loss 0.10468661040067673\n",
      "72 0.10468661040067673\n",
      "73\n",
      "loss 0.10276269167661667\n",
      "73 0.10276269167661667\n",
      "74\n",
      "loss 0.10463704913854599\n",
      "74 0.10463704913854599\n",
      "75\n",
      "loss 0.09870551526546478\n",
      "75 0.09870551526546478\n",
      "76\n",
      "loss 0.1033516526222229\n",
      "76 0.1033516526222229\n",
      "77\n",
      "loss 0.10481853783130646\n",
      "77 0.10481853783130646\n",
      "78\n",
      "loss 0.10247519612312317\n",
      "78 0.10247519612312317\n",
      "79\n",
      "loss 0.10258373618125916\n",
      "79 0.10258373618125916\n",
      "80\n",
      "loss 0.10293187946081161\n",
      "80 0.10293187946081161\n",
      "81\n",
      "loss 0.10089098662137985\n",
      "81 0.10089098662137985\n",
      "82\n",
      "loss 0.10159526020288467\n",
      "82 0.10159526020288467\n",
      "83\n",
      "loss 0.0970010757446289\n",
      "83 0.0970010757446289\n",
      "84\n",
      "loss 0.10113351047039032\n",
      "84 0.10113351047039032\n",
      "85\n",
      "loss 0.10253185033798218\n",
      "85 0.10253185033798218\n",
      "86\n",
      "loss 0.10254072397947311\n",
      "86 0.10254072397947311\n",
      "87\n",
      "loss 0.10242395848035812\n",
      "87 0.10242395848035812\n",
      "88\n",
      "loss 0.09945539385080338\n",
      "88 0.09945539385080338\n",
      "89\n",
      "loss 0.10119521617889404\n",
      "89 0.10119521617889404\n",
      "90\n",
      "loss 0.10095445066690445\n",
      "90 0.10095445066690445\n",
      "91\n",
      "loss 0.097283273935318\n",
      "91 0.097283273935318\n",
      "92\n",
      "loss 0.09840233623981476\n",
      "92 0.09840233623981476\n",
      "93\n",
      "loss 0.1000056266784668\n",
      "93 0.1000056266784668\n",
      "94\n",
      "loss 0.10013270378112793\n",
      "94 0.10013270378112793\n",
      "95\n",
      "loss 0.10127803683280945\n",
      "95 0.10127803683280945\n",
      "96\n",
      "loss 0.09888308495283127\n",
      "96 0.09888308495283127\n",
      "97\n",
      "loss 0.09892427921295166\n",
      "97 0.09892427921295166\n",
      "98\n",
      "loss 0.10169781744480133\n",
      "98 0.10169781744480133\n",
      "99\n",
      "loss 0.09782978892326355\n",
      "99 0.09782978892326355\n",
      "100\n",
      "loss 0.09789235889911652\n",
      "100 0.09789235889911652\n",
      "101\n",
      "loss 0.09810450673103333\n",
      "101 0.09810450673103333\n",
      "102\n",
      "loss 0.09722288697957993\n",
      "102 0.09722288697957993\n",
      "103\n",
      "loss 0.1025824099779129\n",
      "103 0.1025824099779129\n",
      "104\n",
      "loss 0.09743629395961761\n",
      "104 0.09743629395961761\n",
      "105\n",
      "loss 0.09575601667165756\n",
      "105 0.09575601667165756\n",
      "106\n",
      "loss 0.09568413347005844\n",
      "106 0.09568413347005844\n",
      "107\n",
      "loss 0.09444379806518555\n",
      "107 0.09444379806518555\n",
      "108\n",
      "loss 0.09956430643796921\n",
      "108 0.09956430643796921\n",
      "109\n",
      "loss 0.09618936479091644\n",
      "109 0.09618936479091644\n",
      "110\n",
      "loss 0.09546660631895065\n",
      "110 0.09546660631895065\n",
      "111\n",
      "loss 0.09850335866212845\n",
      "111 0.09850335866212845\n",
      "112\n",
      "loss 0.09353435784578323\n",
      "112 0.09353435784578323\n",
      "113\n",
      "loss 0.098037488758564\n",
      "113 0.098037488758564\n",
      "114\n",
      "loss 0.09736622124910355\n",
      "114 0.09736622124910355\n",
      "115\n",
      "loss 0.09506922215223312\n",
      "115 0.09506922215223312\n",
      "116\n",
      "loss 0.09669507294893265\n",
      "116 0.09669507294893265\n",
      "117\n",
      "loss 0.0943324938416481\n",
      "117 0.0943324938416481\n",
      "118\n",
      "loss 0.09584549069404602\n",
      "118 0.09584549069404602\n",
      "119\n",
      "loss 0.09210534393787384\n",
      "119 0.09210534393787384\n",
      "120\n",
      "loss 0.09933266788721085\n",
      "120 0.09933266788721085\n",
      "121\n",
      "loss 0.0942571833729744\n",
      "121 0.0942571833729744\n",
      "122\n",
      "loss 0.09257353097200394\n",
      "122 0.09257353097200394\n",
      "123\n",
      "loss 0.09399794042110443\n",
      "123 0.09399794042110443\n",
      "124\n",
      "loss 0.0943877100944519\n",
      "124 0.0943877100944519\n",
      "125\n",
      "loss 0.09346233308315277\n",
      "125 0.09346233308315277\n",
      "126\n",
      "loss 0.09268888831138611\n",
      "126 0.09268888831138611\n",
      "127\n",
      "loss 0.09919975697994232\n",
      "127 0.09919975697994232\n",
      "128\n",
      "loss 0.09511487931013107\n",
      "128 0.09511487931013107\n",
      "129\n",
      "loss 0.09468863904476166\n",
      "129 0.09468863904476166\n",
      "130\n",
      "loss 0.09635578840970993\n",
      "130 0.09635578840970993\n",
      "131\n",
      "loss 0.0953596979379654\n",
      "131 0.0953596979379654\n",
      "132\n",
      "loss 0.0968373566865921\n",
      "132 0.0968373566865921\n",
      "133\n",
      "loss 0.0952284187078476\n",
      "133 0.0952284187078476\n",
      "134\n",
      "loss 0.09421108663082123\n",
      "134 0.09421108663082123\n",
      "135\n",
      "loss 0.09528866410255432\n",
      "135 0.09528866410255432\n",
      "136\n",
      "loss 0.09284206479787827\n",
      "136 0.09284206479787827\n",
      "137\n",
      "loss 0.09235887974500656\n",
      "137 0.09235887974500656\n",
      "138\n",
      "loss 0.09244478493928909\n",
      "138 0.09244478493928909\n",
      "139\n",
      "loss 0.09230390936136246\n",
      "139 0.09230390936136246\n",
      "140\n",
      "loss 0.0922347903251648\n",
      "140 0.0922347903251648\n",
      "141\n",
      "loss 0.09141963720321655\n",
      "141 0.09141963720321655\n",
      "142\n",
      "loss 0.08903518319129944\n",
      "142 0.08903518319129944\n",
      "143\n",
      "loss 0.08976638317108154\n",
      "143 0.08976638317108154\n",
      "144\n",
      "loss 0.09552666544914246\n",
      "144 0.09552666544914246\n",
      "145\n",
      "loss 0.094260074198246\n",
      "145 0.094260074198246\n",
      "146\n",
      "loss 0.09223894774913788\n",
      "146 0.09223894774913788\n",
      "147\n",
      "loss 0.09218692779541016\n",
      "147 0.09218692779541016\n",
      "148\n",
      "loss 0.0936918556690216\n",
      "148 0.0936918556690216\n",
      "149\n",
      "loss 0.09264276176691055\n",
      "149 0.09264276176691055\n",
      "150\n",
      "loss 0.08912979811429977\n",
      "150 0.08912979811429977\n",
      "151\n",
      "loss 0.09517084062099457\n",
      "151 0.09517084062099457\n",
      "152\n",
      "loss 0.09561984986066818\n",
      "152 0.09561984986066818\n",
      "153\n",
      "loss 0.09054997563362122\n",
      "153 0.09054997563362122\n",
      "154\n",
      "loss 0.091191366314888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154 0.091191366314888\n",
      "155\n",
      "loss 0.08785400539636612\n",
      "155 0.08785400539636612\n",
      "156\n",
      "loss 0.09714507311582565\n",
      "156 0.09714507311582565\n",
      "157\n",
      "loss 0.09180588275194168\n",
      "157 0.09180588275194168\n",
      "158\n",
      "loss 0.09261419624090195\n",
      "158 0.09261419624090195\n",
      "159\n",
      "loss 0.09102822095155716\n",
      "159 0.09102822095155716\n",
      "160\n",
      "loss 0.0888974741101265\n",
      "160 0.0888974741101265\n",
      "161\n",
      "loss 0.09424292296171188\n",
      "161 0.09424292296171188\n",
      "162\n",
      "loss 0.08910277485847473\n",
      "162 0.08910277485847473\n",
      "163\n",
      "loss 0.08785797655582428\n",
      "163 0.08785797655582428\n",
      "164\n",
      "loss 0.09203680604696274\n",
      "164 0.09203680604696274\n",
      "165\n",
      "loss 0.09362271428108215\n",
      "165 0.09362271428108215\n",
      "166\n",
      "loss 0.08867571502923965\n",
      "166 0.08867571502923965\n",
      "167\n",
      "loss 0.09004274755716324\n",
      "167 0.09004274755716324\n",
      "168\n",
      "loss 0.08928389102220535\n",
      "168 0.08928389102220535\n",
      "169\n",
      "loss 0.08885063976049423\n",
      "169 0.08885063976049423\n",
      "170\n",
      "loss 0.09508737921714783\n",
      "170 0.09508737921714783\n",
      "171\n",
      "loss 0.08917352557182312\n",
      "171 0.08917352557182312\n",
      "172\n",
      "loss 0.09548790007829666\n",
      "172 0.09548790007829666\n",
      "173\n",
      "loss 0.08922780305147171\n",
      "173 0.08922780305147171\n",
      "174\n",
      "loss 0.09245369583368301\n",
      "174 0.09245369583368301\n",
      "175\n",
      "loss 0.08854573965072632\n",
      "175 0.08854573965072632\n",
      "176\n",
      "loss 0.08833193778991699\n",
      "176 0.08833193778991699\n",
      "177\n",
      "loss 0.091342493891716\n",
      "177 0.091342493891716\n",
      "178\n",
      "loss 0.0927993506193161\n",
      "178 0.0927993506193161\n",
      "179\n",
      "loss 0.09133613109588623\n",
      "179 0.09133613109588623\n",
      "180\n",
      "loss 0.09121622145175934\n",
      "180 0.09121622145175934\n",
      "181\n",
      "loss 0.0897895023226738\n",
      "181 0.0897895023226738\n",
      "182\n",
      "loss 0.08828303962945938\n",
      "182 0.08828303962945938\n",
      "183\n",
      "loss 0.08790713548660278\n",
      "183 0.08790713548660278\n",
      "184\n",
      "loss 0.08937474340200424\n",
      "184 0.08937474340200424\n",
      "185\n",
      "loss 0.09071311354637146\n",
      "185 0.09071311354637146\n",
      "186\n",
      "loss 0.09009655565023422\n",
      "186 0.09009655565023422\n",
      "187\n",
      "loss 0.09561531245708466\n",
      "187 0.09561531245708466\n",
      "188\n",
      "loss 0.08426633477210999\n",
      "188 0.08426633477210999\n",
      "189\n",
      "loss 0.08968179672956467\n",
      "189 0.08968179672956467\n",
      "190\n",
      "loss 0.08848556131124496\n",
      "190 0.08848556131124496\n",
      "191\n",
      "loss 0.08991213887929916\n",
      "191 0.08991213887929916\n",
      "192\n",
      "loss 0.08874651789665222\n",
      "192 0.08874651789665222\n",
      "193\n",
      "loss 0.08561059832572937\n",
      "193 0.08561059832572937\n",
      "194\n",
      "loss 0.08837546408176422\n",
      "194 0.08837546408176422\n",
      "195\n",
      "loss 0.0899636372923851\n",
      "195 0.0899636372923851\n",
      "196\n",
      "loss 0.08906707167625427\n",
      "196 0.08906707167625427\n",
      "197\n",
      "loss 0.08717229217290878\n",
      "197 0.08717229217290878\n",
      "198\n",
      "loss 0.08850570023059845\n",
      "198 0.08850570023059845\n",
      "199\n",
      "loss 0.08707204461097717\n",
      "199 0.08707204461097717\n",
      "200\n",
      "loss 0.08919435739517212\n",
      "200 0.08919435739517212\n",
      "201\n",
      "loss 0.08577869087457657\n",
      "201 0.08577869087457657\n",
      "202\n",
      "loss 0.08524929732084274\n",
      "202 0.08524929732084274\n",
      "203\n",
      "loss 0.08881663531064987\n",
      "203 0.08881663531064987\n",
      "204\n",
      "loss 0.08681882917881012\n",
      "204 0.08681882917881012\n",
      "205\n",
      "loss 0.0878821611404419\n",
      "205 0.0878821611404419\n",
      "206\n",
      "loss 0.08874905854463577\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "isnan = False\n",
    "while True:\n",
    "    if isnan:\n",
    "        break\n",
    "    for batch in train_loader:\n",
    "        inputs = torch.Tensor(batch['mel']).transpose(1, 2).to(device)\n",
    "        targets = torch.Tensor(batch['blend']).transpose(1, 2).to(device)\n",
    "        \n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs, targets)\n",
    "        \n",
    "        print(step)\n",
    "        loss = 0\n",
    "        for key in outputs.keys():\n",
    "            if 'loss' in key:\n",
    "                loss += outputs[key]\n",
    "                print(key, outputs[key].item())\n",
    "        if torch.isnan(loss):\n",
    "            isnan = True\n",
    "            break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(step, loss.item())\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            writer.add_scalar('train_loss', loss.item(), step)\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            display.clear_output()\n",
    "            \n",
    "            losses = []\n",
    "            for i, batch in enumerate(test_loader):\n",
    "                if i >= 10:\n",
    "                    break\n",
    "                    \n",
    "                inputs = torch.Tensor(batch['mel']).transpose(1, 2).to(device)\n",
    "                targets = torch.Tensor(batch['blend']).transpose(1, 2).to(device)\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs, targets)\n",
    "                    \n",
    "                loss = 0\n",
    "                for key in outputs.keys():\n",
    "                    if 'loss' in key:\n",
    "                        loss += outputs[key]\n",
    "                print('test :', i, loss.item())\n",
    "                losses.append(loss)        \n",
    "            \n",
    "            test_loss = torch.stack(losses).mean().item()\n",
    "            print('test_loss :', test_loss)\n",
    "            writer.add_scalar('test_loss', test_loss, step)\n",
    "            \n",
    "#             plt.figure(figsize=[18, 4])\n",
    "#             librosa.display.specshow(targets[0].data.cpu().numpy(), cmap='magma')\n",
    "#             plt.show()\n",
    "\n",
    "#             plt.figure(figsize=[18, 4])\n",
    "#             librosa.display.specshow(outputs['y_pred'][0].data.cpu().numpy(), cmap='magma')\n",
    "#             plt.show()\n",
    "            \n",
    "#             for i in [20, 37]:\n",
    "#                 plt.figure(figsize=[18, 2])\n",
    "#                 plt.title(str(i))\n",
    "#                 plt.plot(targets[0].data.cpu().numpy()[i])\n",
    "#                 plt.plot(outputs['y_pred'][0].data.cpu().numpy()[i])\n",
    "#                 plt.show()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            save(save_dir, step, model, None, optimizer)\n",
    "    \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c85fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(save_dir, step, model, None, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1166d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce39d126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e142c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ste2",
   "language": "python",
   "name": "ste2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
