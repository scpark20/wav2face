{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79d0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6665403e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 30 02:41:42 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000    Off  | 00000000:1B:00.0 Off |                    0 |\n",
      "| 30%   24C    P8    13W / 230W |  13466MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A5000    Off  | 00000000:1C:00.0 Off |                  Off |\n",
      "| 30%   26C    P8    14W / 230W |    508MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A5000    Off  | 00000000:1D:00.0 Off |                  Off |\n",
      "| 30%   25C    P8    16W / 230W |   1082MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A5000    Off  | 00000000:1E:00.0 Off |                  Off |\n",
      "| 34%   60C    P2   196W / 230W |  21988MiB / 24564MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA RTX A5000    Off  | 00000000:89:00.0 Off |                  Off |\n",
      "| 36%   66C    P2   221W / 230W |  20952MiB / 24564MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA RTX A5000    Off  | 00000000:8A:00.0 Off |                  Off |\n",
      "| 33%   62C    P2   219W / 230W |  21988MiB / 24564MiB |     75%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA RTX A5000    Off  | 00000000:8B:00.0 Off |                  Off |\n",
      "| 30%   41C    P8    19W / 230W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA RTX A5000    Off  | 00000000:8C:00.0 Off |                  Off |\n",
      "| 30%   24C    P8    17W / 230W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A   3490700      C   ...nda3/envs/ste2/bin/python     3118MiB |\n",
      "|    0   N/A  N/A   3551971      C   ...nda3/envs/ste2/bin/python     5178MiB |\n",
      "|    0   N/A  N/A   3599842      C   ...nda3/envs/ste2/bin/python     5162MiB |\n",
      "|    1   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   3551021      C   ...onda3/envs/ste/bin/python      500MiB |\n",
      "|    2   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A     36805      C   ...onda3/envs/ste/bin/python     1074MiB |\n",
      "|    3   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A   3022180      C   ...nda3/envs/ste2/bin/python    21980MiB |\n",
      "|    4   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    4   N/A  N/A   3061306      C   ...nda3/envs/ste2/bin/python    20944MiB |\n",
      "|    5   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    5   N/A  N/A    287500      C   ...nda3/envs/ste2/bin/python    21980MiB |\n",
      "|    6   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    7   N/A  N/A      3595      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "!nvidia-smi\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee05e12",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb821737",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 1024\n",
    "n_outputs = 61\n",
    "n_frames = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c54e68",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cedcf798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from model.model_ecapa_transformer_reg import Model\n",
    "from utils.util import *\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "step = 0\n",
    "device = 'cuda:0'\n",
    "\n",
    "# Model\n",
    "model = Model(in_dim=n_mels, h_dim=512, out_dim=n_outputs)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31748b8c",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998d7f8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5299804\r\n",
      "-rw-rw-r-- 1 scpark scpark     61431  8월 30 02:41 events.out.tfevents.1693235571.GPUSVR01\r\n",
      "-rw-rw-r-- 1 scpark scpark 602984328  8월 30 02:00 save_70000\r\n",
      "-rw-rw-r-- 1 scpark scpark 602984328  8월 29 21:44 save_60000\r\n",
      "-rw-rw-r-- 1 scpark scpark 602984328  8월 29 17:29 save_50000\r\n",
      "-rw-rw-r-- 1 scpark scpark 602984328  8월 29 13:14 save_40000\r\n",
      "-rw-rw-r-- 1 scpark scpark 602984328  8월 29 08:59 save_30000\r\n",
      "-rw-rw-r-- 1 scpark scpark 602984328  8월 29 04:43 save_20000\r\n",
      "-rw-rw-r-- 1 scpark scpark 602984328  8월 29 00:28 save_10000\r\n",
      "-rw-rw-r-- 1 scpark scpark 602983560  8월 29 00:12 save_9474\r\n",
      "-rw-rw-r-- 1 scpark scpark      9247  8월 29 00:10 events.out.tfevents.1693220571.GPUSVR01\r\n",
      "-rw-rw-r-- 1 scpark scpark 602983560  8월 28 20:04 save_0\r\n",
      "-rw-rw-r-- 1 scpark scpark        40  8월 28 20:02 events.out.tfevents.1693220464.GPUSVR01\r\n",
      "-rw-rw-r-- 1 scpark scpark      2263  8월 28 20:00 events.out.tfevents.1693216947.GPUSVR01\r\n",
      "-rw-rw-r-- 1 scpark scpark       131  8월 28 18:46 events.out.tfevents.1693215873.GPUSVR01\r\n",
      "-rw-rw-r-- 1 scpark scpark       131  8월 28 18:44 events.out.tfevents.1693215749.GPUSVR01\r\n",
      "-rw-rw-r-- 1 scpark scpark        40  8월 28 18:42 events.out.tfevents.1693215719.GPUSVR01\r\n",
      "-rw-rw-r-- 1 scpark scpark       131  8월 28 18:40 events.out.tfevents.1693215284.GPUSVR01\r\n"
     ]
    }
   ],
   "source": [
    "save_dir = '/data/scpark/save/lips/train08.28-4/'\n",
    "!mkdir -p $save_dir\n",
    "!ls -lt $save_dir\n",
    "\n",
    "writer = SummaryWriter(save_dir)\n",
    "\n",
    "if False:\n",
    "    step, model, _, optimizer = load(save_dir, 7321, model, None, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5e7b44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events.out.tfevents.1693214128.GPUSVR01  save_0      save_20000\r\n",
      "events.out.tfevents.1693236587.GPUSVR01  save_10000  save_30000\r\n"
     ]
    }
   ],
   "source": [
    "!ls /data/scpark/save/lips/train08.28-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2742e6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker_embedding.blocks.0.conv.conv.weight\n",
      "speaker_embedding.blocks.0.conv.conv.bias\n",
      "speaker_embedding.blocks.0.norm.norm.weight\n",
      "speaker_embedding.blocks.0.norm.norm.bias\n",
      "speaker_embedding.blocks.0.norm.norm.running_mean\n",
      "speaker_embedding.blocks.0.norm.norm.running_var\n",
      "speaker_embedding.blocks.0.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.tdnn1.conv.conv.weight\n",
      "speaker_embedding.blocks.1.tdnn1.conv.conv.bias\n",
      "speaker_embedding.blocks.1.tdnn1.norm.norm.weight\n",
      "speaker_embedding.blocks.1.tdnn1.norm.norm.bias\n",
      "speaker_embedding.blocks.1.tdnn1.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.tdnn1.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.tdnn1.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.0.conv.conv.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.0.conv.conv.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.0.norm.norm.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.0.norm.norm.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.0.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.0.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.0.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.1.conv.conv.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.1.conv.conv.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.1.norm.norm.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.1.norm.norm.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.1.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.1.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.1.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.2.conv.conv.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.2.conv.conv.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.2.norm.norm.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.2.norm.norm.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.2.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.2.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.2.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.3.conv.conv.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.3.conv.conv.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.3.norm.norm.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.3.norm.norm.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.3.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.3.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.3.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.4.conv.conv.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.4.conv.conv.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.4.norm.norm.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.4.norm.norm.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.4.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.4.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.4.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.5.conv.conv.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.5.conv.conv.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.5.norm.norm.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.5.norm.norm.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.5.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.5.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.5.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.6.conv.conv.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.6.conv.conv.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.6.norm.norm.weight\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.6.norm.norm.bias\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.6.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.6.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.res2net_block.blocks.6.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.tdnn2.conv.conv.weight\n",
      "speaker_embedding.blocks.1.tdnn2.conv.conv.bias\n",
      "speaker_embedding.blocks.1.tdnn2.norm.norm.weight\n",
      "speaker_embedding.blocks.1.tdnn2.norm.norm.bias\n",
      "speaker_embedding.blocks.1.tdnn2.norm.norm.running_mean\n",
      "speaker_embedding.blocks.1.tdnn2.norm.norm.running_var\n",
      "speaker_embedding.blocks.1.tdnn2.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.1.se_block.conv1.conv.weight\n",
      "speaker_embedding.blocks.1.se_block.conv1.conv.bias\n",
      "speaker_embedding.blocks.1.se_block.conv2.conv.weight\n",
      "speaker_embedding.blocks.1.se_block.conv2.conv.bias\n",
      "speaker_embedding.blocks.2.tdnn1.conv.conv.weight\n",
      "speaker_embedding.blocks.2.tdnn1.conv.conv.bias\n",
      "speaker_embedding.blocks.2.tdnn1.norm.norm.weight\n",
      "speaker_embedding.blocks.2.tdnn1.norm.norm.bias\n",
      "speaker_embedding.blocks.2.tdnn1.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.tdnn1.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.tdnn1.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.0.conv.conv.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.0.conv.conv.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.0.norm.norm.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.0.norm.norm.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.0.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.0.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.0.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.1.conv.conv.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.1.conv.conv.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.1.norm.norm.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.1.norm.norm.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.1.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.1.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.1.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.2.conv.conv.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.2.conv.conv.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.2.norm.norm.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.2.norm.norm.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.2.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.2.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.2.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.3.conv.conv.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.3.conv.conv.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.3.norm.norm.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.3.norm.norm.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.3.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.3.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.3.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.4.conv.conv.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.4.conv.conv.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.4.norm.norm.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.4.norm.norm.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.4.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.4.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.4.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.5.conv.conv.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.5.conv.conv.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.5.norm.norm.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.5.norm.norm.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.5.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.5.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.5.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.6.conv.conv.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.6.conv.conv.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.6.norm.norm.weight\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.6.norm.norm.bias\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.6.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.6.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.res2net_block.blocks.6.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.tdnn2.conv.conv.weight\n",
      "speaker_embedding.blocks.2.tdnn2.conv.conv.bias\n",
      "speaker_embedding.blocks.2.tdnn2.norm.norm.weight\n",
      "speaker_embedding.blocks.2.tdnn2.norm.norm.bias\n",
      "speaker_embedding.blocks.2.tdnn2.norm.norm.running_mean\n",
      "speaker_embedding.blocks.2.tdnn2.norm.norm.running_var\n",
      "speaker_embedding.blocks.2.tdnn2.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.2.se_block.conv1.conv.weight\n",
      "speaker_embedding.blocks.2.se_block.conv1.conv.bias\n",
      "speaker_embedding.blocks.2.se_block.conv2.conv.weight\n",
      "speaker_embedding.blocks.2.se_block.conv2.conv.bias\n",
      "speaker_embedding.blocks.3.tdnn1.conv.conv.weight\n",
      "speaker_embedding.blocks.3.tdnn1.conv.conv.bias\n",
      "speaker_embedding.blocks.3.tdnn1.norm.norm.weight\n",
      "speaker_embedding.blocks.3.tdnn1.norm.norm.bias\n",
      "speaker_embedding.blocks.3.tdnn1.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.tdnn1.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.tdnn1.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.0.conv.conv.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.0.conv.conv.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.0.norm.norm.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.0.norm.norm.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.0.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.0.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.0.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.1.conv.conv.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.1.conv.conv.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.1.norm.norm.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.1.norm.norm.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.1.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.1.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.1.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.2.conv.conv.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.2.conv.conv.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.2.norm.norm.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.2.norm.norm.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.2.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.2.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.2.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.3.conv.conv.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.3.conv.conv.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.3.norm.norm.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.3.norm.norm.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.3.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.3.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.3.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.4.conv.conv.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.4.conv.conv.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.4.norm.norm.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.4.norm.norm.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.4.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.4.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.4.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.5.conv.conv.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.5.conv.conv.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.5.norm.norm.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.5.norm.norm.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.5.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.5.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.5.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.6.conv.conv.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.6.conv.conv.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.6.norm.norm.weight\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.6.norm.norm.bias\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.6.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.6.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.res2net_block.blocks.6.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.tdnn2.conv.conv.weight\n",
      "speaker_embedding.blocks.3.tdnn2.conv.conv.bias\n",
      "speaker_embedding.blocks.3.tdnn2.norm.norm.weight\n",
      "speaker_embedding.blocks.3.tdnn2.norm.norm.bias\n",
      "speaker_embedding.blocks.3.tdnn2.norm.norm.running_mean\n",
      "speaker_embedding.blocks.3.tdnn2.norm.norm.running_var\n",
      "speaker_embedding.blocks.3.tdnn2.norm.norm.num_batches_tracked\n",
      "speaker_embedding.blocks.3.se_block.conv1.conv.weight\n",
      "speaker_embedding.blocks.3.se_block.conv1.conv.bias\n",
      "speaker_embedding.blocks.3.se_block.conv2.conv.weight\n",
      "speaker_embedding.blocks.3.se_block.conv2.conv.bias\n",
      "speaker_embedding.mfa.conv.conv.weight\n",
      "speaker_embedding.mfa.conv.conv.bias\n",
      "speaker_embedding.mfa.norm.norm.weight\n",
      "speaker_embedding.mfa.norm.norm.bias\n",
      "speaker_embedding.mfa.norm.norm.running_mean\n",
      "speaker_embedding.mfa.norm.norm.running_var\n",
      "speaker_embedding.mfa.norm.norm.num_batches_tracked\n",
      "speaker_embedding.asp.tdnn.conv.conv.weight\n",
      "speaker_embedding.asp.tdnn.conv.conv.bias\n",
      "speaker_embedding.asp.tdnn.norm.norm.weight\n",
      "speaker_embedding.asp.tdnn.norm.norm.bias\n",
      "speaker_embedding.asp.tdnn.norm.norm.running_mean\n",
      "speaker_embedding.asp.tdnn.norm.norm.running_var\n",
      "speaker_embedding.asp.tdnn.norm.norm.num_batches_tracked\n",
      "speaker_embedding.asp.conv.conv.weight\n",
      "speaker_embedding.asp.conv.conv.bias\n",
      "speaker_embedding.asp_bn.norm.weight\n",
      "speaker_embedding.asp_bn.norm.bias\n",
      "speaker_embedding.asp_bn.norm.running_mean\n",
      "speaker_embedding.asp_bn.norm.running_var\n",
      "speaker_embedding.asp_bn.norm.num_batches_tracked\n",
      "speaker_embedding.fc.conv.weight\n",
      "speaker_embedding.fc.conv.bias\n",
      "speaker_linear.weight\n",
      "speaker_linear.bias\n",
      "prenet.weight\n",
      "prenet.bias\n",
      "encoder.attn_layers.0.emb_rel_k\n",
      "encoder.attn_layers.0.emb_rel_v\n",
      "encoder.attn_layers.0.conv_q.weight\n",
      "encoder.attn_layers.0.conv_q.bias\n",
      "encoder.attn_layers.0.conv_k.weight\n",
      "encoder.attn_layers.0.conv_k.bias\n",
      "encoder.attn_layers.0.conv_v.weight\n",
      "encoder.attn_layers.0.conv_v.bias\n",
      "encoder.attn_layers.0.conv_o.weight\n",
      "encoder.attn_layers.0.conv_o.bias\n",
      "encoder.attn_layers.1.emb_rel_k\n",
      "encoder.attn_layers.1.emb_rel_v\n",
      "encoder.attn_layers.1.conv_q.weight\n",
      "encoder.attn_layers.1.conv_q.bias\n",
      "encoder.attn_layers.1.conv_k.weight\n",
      "encoder.attn_layers.1.conv_k.bias\n",
      "encoder.attn_layers.1.conv_v.weight\n",
      "encoder.attn_layers.1.conv_v.bias\n",
      "encoder.attn_layers.1.conv_o.weight\n",
      "encoder.attn_layers.1.conv_o.bias\n",
      "encoder.attn_layers.2.emb_rel_k\n",
      "encoder.attn_layers.2.emb_rel_v\n",
      "encoder.attn_layers.2.conv_q.weight\n",
      "encoder.attn_layers.2.conv_q.bias\n",
      "encoder.attn_layers.2.conv_k.weight\n",
      "encoder.attn_layers.2.conv_k.bias\n",
      "encoder.attn_layers.2.conv_v.weight\n",
      "encoder.attn_layers.2.conv_v.bias\n",
      "encoder.attn_layers.2.conv_o.weight\n",
      "encoder.attn_layers.2.conv_o.bias\n",
      "encoder.attn_layers.3.emb_rel_k\n",
      "encoder.attn_layers.3.emb_rel_v\n",
      "encoder.attn_layers.3.conv_q.weight\n",
      "encoder.attn_layers.3.conv_q.bias\n",
      "encoder.attn_layers.3.conv_k.weight\n",
      "encoder.attn_layers.3.conv_k.bias\n",
      "encoder.attn_layers.3.conv_v.weight\n",
      "encoder.attn_layers.3.conv_v.bias\n",
      "encoder.attn_layers.3.conv_o.weight\n",
      "encoder.attn_layers.3.conv_o.bias\n",
      "encoder.attn_layers.4.emb_rel_k\n",
      "encoder.attn_layers.4.emb_rel_v\n",
      "encoder.attn_layers.4.conv_q.weight\n",
      "encoder.attn_layers.4.conv_q.bias\n",
      "encoder.attn_layers.4.conv_k.weight\n",
      "encoder.attn_layers.4.conv_k.bias\n",
      "encoder.attn_layers.4.conv_v.weight\n",
      "encoder.attn_layers.4.conv_v.bias\n",
      "encoder.attn_layers.4.conv_o.weight\n",
      "encoder.attn_layers.4.conv_o.bias\n",
      "encoder.attn_layers.5.emb_rel_k\n",
      "encoder.attn_layers.5.emb_rel_v\n",
      "encoder.attn_layers.5.conv_q.weight\n",
      "encoder.attn_layers.5.conv_q.bias\n",
      "encoder.attn_layers.5.conv_k.weight\n",
      "encoder.attn_layers.5.conv_k.bias\n",
      "encoder.attn_layers.5.conv_v.weight\n",
      "encoder.attn_layers.5.conv_v.bias\n",
      "encoder.attn_layers.5.conv_o.weight\n",
      "encoder.attn_layers.5.conv_o.bias\n",
      "encoder.norm_layers_1.0.gamma\n",
      "encoder.norm_layers_1.0.beta\n",
      "encoder.norm_layers_1.1.gamma\n",
      "encoder.norm_layers_1.1.beta\n",
      "encoder.norm_layers_1.2.gamma\n",
      "encoder.norm_layers_1.2.beta\n",
      "encoder.norm_layers_1.3.gamma\n",
      "encoder.norm_layers_1.3.beta\n",
      "encoder.norm_layers_1.4.gamma\n",
      "encoder.norm_layers_1.4.beta\n",
      "encoder.norm_layers_1.5.gamma\n",
      "encoder.norm_layers_1.5.beta\n",
      "encoder.ffn_layers.0.conv_1.weight\n",
      "encoder.ffn_layers.0.conv_1.bias\n",
      "encoder.ffn_layers.0.conv_2.weight\n",
      "encoder.ffn_layers.0.conv_2.bias\n",
      "encoder.ffn_layers.1.conv_1.weight\n",
      "encoder.ffn_layers.1.conv_1.bias\n",
      "encoder.ffn_layers.1.conv_2.weight\n",
      "encoder.ffn_layers.1.conv_2.bias\n",
      "encoder.ffn_layers.2.conv_1.weight\n",
      "encoder.ffn_layers.2.conv_1.bias\n",
      "encoder.ffn_layers.2.conv_2.weight\n",
      "encoder.ffn_layers.2.conv_2.bias\n",
      "encoder.ffn_layers.3.conv_1.weight\n",
      "encoder.ffn_layers.3.conv_1.bias\n",
      "encoder.ffn_layers.3.conv_2.weight\n",
      "encoder.ffn_layers.3.conv_2.bias\n",
      "encoder.ffn_layers.4.conv_1.weight\n",
      "encoder.ffn_layers.4.conv_1.bias\n",
      "encoder.ffn_layers.4.conv_2.weight\n",
      "encoder.ffn_layers.4.conv_2.bias\n",
      "encoder.ffn_layers.5.conv_1.weight\n",
      "encoder.ffn_layers.5.conv_1.bias\n",
      "encoder.ffn_layers.5.conv_2.weight\n",
      "encoder.ffn_layers.5.conv_2.bias\n",
      "encoder.norm_layers_2.0.gamma\n",
      "encoder.norm_layers_2.0.beta\n",
      "encoder.norm_layers_2.1.gamma\n",
      "encoder.norm_layers_2.1.beta\n",
      "encoder.norm_layers_2.2.gamma\n",
      "encoder.norm_layers_2.2.beta\n",
      "encoder.norm_layers_2.3.gamma\n",
      "encoder.norm_layers_2.3.beta\n",
      "encoder.norm_layers_2.4.gamma\n",
      "encoder.norm_layers_2.4.beta\n",
      "encoder.norm_layers_2.5.gamma\n",
      "encoder.norm_layers_2.5.beta\n",
      "postnet.weight\n",
      "postnet.bias\n",
      "warm start\n"
     ]
    }
   ],
   "source": [
    "# # warm start\n",
    "# checkpoint = torch.load('/data/scpark/save/lips/train08.28-1/save_30000', map_location=torch.device('cpu'))\n",
    "# model_state_dict = model.state_dict()\n",
    "\n",
    "# for key in checkpoint['model_state_dict']:\n",
    "#     if key in model_state_dict.keys():\n",
    "#         if checkpoint['model_state_dict'][key].shape == model_state_dict[key].shape:\n",
    "#             model_state_dict[key] = checkpoint['model_state_dict'][key]\n",
    "#             print(key)\n",
    "# model.load_state_dict(model_state_dict, strict=True)\n",
    "# print('warm start')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71d7b3c",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a8aa62c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_10_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_11_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_12_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_1_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_2_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_3_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_4_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_5_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_8_iPhone_raw.npy\n",
      "1 /data/speech/digital_human/preprocessed/jeewonPark/MH_ARKit_003_9_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_10_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_1_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_2_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_3_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_4_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_5_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_6_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_7_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_8_iPhone_raw.npy\n",
      "2 /data/speech/digital_human/preprocessed/jinwooOh/MH_ARKit_005_9_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_10_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_1_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_2_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_3_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_4_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_5_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_6_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_7_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_8_iPhone_raw.npy\n",
      "3 /data/speech/digital_human/preprocessed/kyuchulLee/MH_ARKit_006_9_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_10_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_1_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_2_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_3_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_4_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_5_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_6_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_7_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_8_iPhone_raw.npy\n",
      "4 /data/speech/digital_human/preprocessed/kyuseokKim/MH_ARKit_002_9_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_0_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_1_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_2_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_3_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_4_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_5_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_6_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_7_iPhone_raw.npy\n",
      "5 /data/speech/digital_human/preprocessed/nohsikPark/MH_ARKit_010_8_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_10_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_1_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_2_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_3_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_4_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_5_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_6_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_7_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_8_iPhone_raw.npy\n",
      "6 /data/speech/digital_human/preprocessed/soochulPark/MH_ARKit_004_9_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_1_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_20_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_21_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_2_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_30_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_31_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_3_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_40_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_4_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_50_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_51_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_52_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_5_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_6_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_7_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_8_iPhone_raw.npy\n",
      "7 /data/speech/digital_human/preprocessed/yehunHwang/MH_ARKit_001_9_iPhone_raw.npy\n",
      "71 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from data.arkit_dataset import LipsDataset, CombinedDataset, CombinedCollate\n",
    "\n",
    "root_dir = '/data/speech/digital_human/preprocessed/'\n",
    "\n",
    "def get_files(dir):\n",
    "    data = []\n",
    "    files = sorted([os.path.join(dir, file) for file in os.listdir(dir)])\n",
    "    for file in files:\n",
    "        if file.endswith('.npy') and 'ARKit' in file:\n",
    "            data.append(file)\n",
    "        if os.path.isdir(file):\n",
    "            data.extend(get_files(os.path.join(dir, file)))\n",
    "    return data\n",
    "\n",
    "files = get_files(root_dir)\n",
    "print(len(files))\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "def get_sid(file):\n",
    "    names = ['No Speaker', 'jeewonPark', 'jinwooOh', 'kyuchulLee', 'kyuseokKim', 'nohsikPark', 'soochulPark', 'yehunHwang']\n",
    "    for sid, name in enumerate(names):\n",
    "        if name in file:\n",
    "            return sid\n",
    "    return 0\n",
    "\n",
    "for file in files:\n",
    "    sid = get_sid(file)\n",
    "    print(sid, file)\n",
    "    dataset = LipsDataset(file, n_mels, n_frames, sid=sid, mel=False)\n",
    "    if '_10_' in file:\n",
    "        test_datasets.append(dataset)\n",
    "    else:\n",
    "        train_datasets.append(dataset)\n",
    "print(len(train_datasets), len(test_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bc7cac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(CombinedDataset(train_datasets), \n",
    "                                           num_workers=16, shuffle=True, batch_size=32, collate_fn=CombinedCollate())\n",
    "test_loader = torch.utils.data.DataLoader(CombinedDataset(test_datasets), \n",
    "                                          num_workers=10, shuffle=True, batch_size=10, collate_fn=CombinedCollate())\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a05691c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "wav2vec = Wav2Vec2ForCTC.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\").to(device)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c5a57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Resample\n",
    "resample = Resample(24000, 16000).to(device)\n",
    "\n",
    "def get_states(wav, size, layer=4):\n",
    "    # 24k to 16k\n",
    "    wav = resample(wav)\n",
    "    # Get probs.\n",
    "    with torch.no_grad():\n",
    "        states = wav2vec(wav, output_hidden_states=True).hidden_states[layer].transpose(1, 2)\n",
    "    # Resizing\n",
    "    states = F.interpolate(states, size=size, mode='linear').detach()\n",
    "    return states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03238ce2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0 0.023232290521264076\n",
      "test : 1 0.026242315769195557\n",
      "test : 2 0.026224516332149506\n",
      "test : 3 0.027755634859204292\n",
      "test : 4 0.02350422739982605\n",
      "test : 5 0.022468676790595055\n",
      "test : 6 0.027570931240916252\n",
      "test : 7 0.02534586191177368\n",
      "test : 8 0.02641645073890686\n",
      "test : 9 0.02612779103219509\n",
      "test_loss : 0.025488872081041336\n",
      "saved /data/scpark/save/lips/train08.28-4/save_0\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "isnan = False\n",
    "while True:\n",
    "    if isnan:\n",
    "        break\n",
    "    for batch in train_loader:\n",
    "        targets = torch.Tensor(batch['blend']).transpose(1, 2).to(device)\n",
    "        inputs = get_states(torch.Tensor(batch['wav']).to(device), targets.shape[-1])\n",
    "        sid = torch.Tensor(batch['sid']).int().to(device)\n",
    "        \n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs, targets, sid)\n",
    "        \n",
    "        print(step)\n",
    "        loss = 0\n",
    "        for key in outputs.keys():\n",
    "            if 'loss' in key:\n",
    "                loss += outputs[key]\n",
    "                print(key, outputs[key].item())\n",
    "        if torch.isnan(loss):\n",
    "            isnan = True\n",
    "            break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(step, loss.item())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            writer.add_scalar('train_loss', loss.item(), step)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            display.clear_output()\n",
    "            \n",
    "            # (b, 2)\n",
    "            speaker = outputs['speaker'].data.cpu().numpy()\n",
    "            plt.figure(figsize=[8, 8])\n",
    "            plt.scatter(speaker[:, 0], speaker[:, 1])\n",
    "            plt.show()\n",
    "            \n",
    "            losses = []\n",
    "            for i, batch in enumerate(test_loader):\n",
    "                if i >= 10:\n",
    "                    break\n",
    "                    \n",
    "                targets = torch.Tensor(batch['blend']).transpose(1, 2).to(device)\n",
    "                inputs = get_states(torch.Tensor(batch['wav']).to(device), targets.shape[-1])\n",
    "                sid = torch.Tensor(batch['sid']).int().to(device)\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs, targets, sid)\n",
    "                    \n",
    "                loss = 0\n",
    "                for key in outputs.keys():\n",
    "                    if 'loss' in key:\n",
    "                        loss += outputs[key]\n",
    "                print('test :', i, loss.item())\n",
    "                losses.append(loss)        \n",
    "            \n",
    "            test_loss = torch.stack(losses).mean().item()\n",
    "            print('test_loss :', test_loss)\n",
    "            writer.add_scalar('test_loss', test_loss, step)\n",
    "            \n",
    "#             plt.figure(figsize=[18, 4])\n",
    "#             librosa.display.specshow(targets[0].data.cpu().numpy(), cmap='magma')\n",
    "#             plt.show()\n",
    "\n",
    "#             plt.figure(figsize=[18, 4])\n",
    "#             librosa.display.specshow(outputs['y_pred'][0].data.cpu().numpy(), cmap='magma')\n",
    "#             plt.show()\n",
    "            \n",
    "#             for i in [20, 37]:\n",
    "#                 plt.figure(figsize=[18, 2])\n",
    "#                 plt.title(str(i))\n",
    "#                 plt.plot(targets[0].data.cpu().numpy()[i])\n",
    "#                 plt.plot(outputs['y_pred'][0].data.cpu().numpy()[i])\n",
    "#                 plt.show()\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            save(save_dir, step, model, None, optimizer)\n",
    "    \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c85fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(save_dir, step, model, None, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35939c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (b, 2)\n",
    "speaker = outputs['speaker'].data.cpu().numpy()\n",
    "plt.figure(figsize=[8, 8])\n",
    "plt.scatter(speaker[:, 0], speaker[:, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08695381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2467159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e1031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
