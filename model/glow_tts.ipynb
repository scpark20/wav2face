{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aafb4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def squeeze(x, x_mask=None, n_sqz=2):\n",
    "  b, c, t = x.size()\n",
    "\n",
    "  t = (t // n_sqz) * n_sqz\n",
    "  x = x[:,:,:t]\n",
    "  x_sqz = x.view(b, c, t//n_sqz, n_sqz)\n",
    "  x_sqz = x_sqz.permute(0, 3, 1, 2).contiguous().view(b, c*n_sqz, t//n_sqz)\n",
    "  \n",
    "  if x_mask is not None:\n",
    "    x_mask = x_mask[:,:,n_sqz-1::n_sqz]\n",
    "  else:\n",
    "    x_mask = torch.ones(b, 1, t//n_sqz).to(device=x.device, dtype=x.dtype)\n",
    "  return x_sqz * x_mask, x_mask\n",
    "\n",
    "\n",
    "def unsqueeze(x, x_mask=None, n_sqz=2):\n",
    "  b, c, t = x.size()\n",
    "\n",
    "  x_unsqz = x.view(b, n_sqz, c//n_sqz, t)\n",
    "  x_unsqz = x_unsqz.permute(0, 2, 3, 1).contiguous().view(b, c//n_sqz, t*n_sqz)\n",
    "\n",
    "  if x_mask is not None:\n",
    "    x_mask = x_mask.unsqueeze(-1).repeat(1,1,1,n_sqz).view(b, 1, t*n_sqz)\n",
    "  else:\n",
    "    x_mask = torch.ones(b, 1, t*n_sqz).to(device=x.device, dtype=x.dtype)\n",
    "  return x_unsqz * x_mask, \n",
    "\n",
    "class ActNorm(nn.Module):\n",
    "  def __init__(self, channels, ddi=False, **kwargs):\n",
    "    super().__init__()\n",
    "    self.channels = channels\n",
    "    self.initialized = not ddi\n",
    "\n",
    "    self.logs = nn.Parameter(torch.zeros(1, channels, 1))\n",
    "    self.bias = nn.Parameter(torch.zeros(1, channels, 1))\n",
    "\n",
    "  def forward(self, x, x_mask=None, reverse=False, **kwargs):\n",
    "    if x_mask is None:\n",
    "      x_mask = torch.ones(x.size(0), 1, x.size(2)).to(device=x.device, dtype=x.dtype)\n",
    "    x_len = torch.sum(x_mask, [1, 2])\n",
    "    if not self.initialized:\n",
    "      self.initialize(x, x_mask)\n",
    "      self.initialized = True\n",
    "\n",
    "    if reverse:\n",
    "      z = (x - self.bias) * torch.exp(-self.logs) * x_mask\n",
    "      logdet = None\n",
    "    else:\n",
    "      z = (self.bias + torch.exp(self.logs) * x) * x_mask\n",
    "      logdet = torch.sum(self.logs) * x_len # [b]\n",
    "\n",
    "    return z, logdet\n",
    "\n",
    "  def store_inverse(self):\n",
    "    pass\n",
    "\n",
    "  def set_ddi(self, ddi):\n",
    "    self.initialized = not ddi\n",
    "\n",
    "  def initialize(self, x, x_mask):\n",
    "    with torch.no_grad():\n",
    "      denom = torch.sum(x_mask, [0, 2])\n",
    "      m = torch.sum(x * x_mask, [0, 2]) / denom\n",
    "      m_sq = torch.sum(x * x * x_mask, [0, 2]) / denom\n",
    "      v = m_sq - (m ** 2)\n",
    "      logs = 0.5 * torch.log(torch.clamp_min(v, 1e-6))\n",
    "\n",
    "      bias_init = (-m * torch.exp(-logs)).view(*self.bias.shape).to(dtype=self.bias.dtype)\n",
    "      logs_init = (-logs).view(*self.logs.shape).to(dtype=self.logs.dtype)\n",
    "\n",
    "      self.bias.data.copy_(bias_init)\n",
    "      self.logs.data.copy_(logs_init)\n",
    "    \n",
    "class InvConvNear(nn.Module):\n",
    "  def __init__(self, channels, n_split=4, no_jacobian=False, **kwargs):\n",
    "    super().__init__()\n",
    "    assert(n_split % 2 == 0)\n",
    "    self.channels = channels\n",
    "    self.n_split = n_split\n",
    "    self.no_jacobian = no_jacobian\n",
    "    \n",
    "    w_init = torch.qr(torch.FloatTensor(self.n_split, self.n_split).normal_())[0]\n",
    "    if torch.det(w_init) < 0:\n",
    "      w_init[:,0] = -1 * w_init[:,0]\n",
    "    self.weight = nn.Parameter(w_init)\n",
    "\n",
    "  def forward(self, x, x_mask=None, reverse=False, **kwargs):\n",
    "    b, c, t = x.size()\n",
    "    assert(c % self.n_split == 0)\n",
    "    if x_mask is None:\n",
    "      x_mask = 1\n",
    "      x_len = torch.ones((b,), dtype=x.dtype, device=x.device) * t\n",
    "    else:\n",
    "      x_len = torch.sum(x_mask, [1, 2])\n",
    "\n",
    "    x = x.view(b, 2, c // self.n_split, self.n_split // 2, t)\n",
    "    x = x.permute(0, 1, 3, 2, 4).contiguous().view(b, self.n_split, c // self.n_split, t)\n",
    "\n",
    "    if reverse:\n",
    "      if hasattr(self, \"weight_inv\"):\n",
    "        weight = self.weight_inv\n",
    "      else:\n",
    "        weight = torch.inverse(self.weight.float()).to(dtype=self.weight.dtype)\n",
    "      logdet = None\n",
    "    else:\n",
    "      weight = self.weight\n",
    "      if self.no_jacobian:\n",
    "        logdet = 0\n",
    "      else:\n",
    "        logdet = torch.logdet(self.weight) * (c / self.n_split) * x_len # [b]\n",
    "\n",
    "    weight = weight.view(self.n_split, self.n_split, 1, 1)\n",
    "    z = F.conv2d(x, weight)\n",
    "\n",
    "    z = z.view(b, 2, self.n_split // 2, c // self.n_split, t)\n",
    "    z = z.permute(0, 1, 3, 2, 4).contiguous().view(b, c, t) * x_mask\n",
    "    return z, logdet\n",
    "\n",
    "  def store_inverse(self):\n",
    "    self.weight_inv = torch.inverse(self.weight.float()).to(dtype=self.weight.dtype)\n",
    "    \n",
    "class CouplingBlock(nn.Module):\n",
    "  def __init__(self, in_channels, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=0, p_dropout=0, sigmoid_scale=False):\n",
    "    super().__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.hidden_channels = hidden_channels\n",
    "    self.kernel_size = kernel_size\n",
    "    self.dilation_rate = dilation_rate\n",
    "    self.n_layers = n_layers\n",
    "    self.gin_channels = gin_channels\n",
    "    self.p_dropout = p_dropout\n",
    "    self.sigmoid_scale = sigmoid_scale\n",
    "\n",
    "    start = torch.nn.Conv1d(in_channels//2, hidden_channels, 1)\n",
    "    start = torch.nn.utils.weight_norm(start)\n",
    "    self.start = start\n",
    "    # Initializing last layer to 0 makes the affine coupling layers\n",
    "    # do nothing at first.  It helps to stabilze training.\n",
    "    end = torch.nn.Conv1d(hidden_channels, in_channels, 1)\n",
    "    end.weight.data.zero_()\n",
    "    end.bias.data.zero_()\n",
    "    self.end = end\n",
    "\n",
    "    self.wn = modules.WN(in_channels, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels, p_dropout)\n",
    "\n",
    "\n",
    "  def forward(self, x, x_mask=None, reverse=False, g=None, **kwargs):\n",
    "    b, c, t = x.size()\n",
    "    if x_mask is None:\n",
    "      x_mask = 1\n",
    "    x_0, x_1 = x[:,:self.in_channels//2], x[:,self.in_channels//2:]\n",
    "\n",
    "    x = self.start(x_0) * x_mask\n",
    "    x = self.wn(x, x_mask, g)\n",
    "    out = self.end(x)\n",
    "\n",
    "    z_0 = x_0\n",
    "    m = out[:, :self.in_channels//2, :]\n",
    "    logs = out[:, self.in_channels//2:, :]\n",
    "    if self.sigmoid_scale:\n",
    "      logs = torch.log(1e-6 + torch.sigmoid(logs + 2))\n",
    "\n",
    "    if reverse:\n",
    "      z_1 = (x_1 - m) * torch.exp(-logs) * x_mask\n",
    "      logdet = None\n",
    "    else:\n",
    "      z_1 = (m + torch.exp(logs) * x_1) * x_mask\n",
    "      logdet = torch.sum(logs * x_mask, [1, 2])\n",
    "\n",
    "    z = torch.cat([z_0, z_1], 1)\n",
    "    return z, logdet\n",
    "\n",
    "  def store_inverse(self):\n",
    "    self.wn.remove_weight_norm()\n",
    "\n",
    "class FlowSpecDecoder(nn.Module):\n",
    "  def __init__(self, \n",
    "      in_channels, \n",
    "      hidden_channels, \n",
    "      kernel_size, \n",
    "      dilation_rate, \n",
    "      n_blocks, \n",
    "      n_layers, \n",
    "      p_dropout=0., \n",
    "      n_split=4,\n",
    "      n_sqz=2,\n",
    "      sigmoid_scale=False,\n",
    "      gin_channels=0):\n",
    "    super().__init__()\n",
    "\n",
    "    self.in_channels = in_channels\n",
    "    self.hidden_channels = hidden_channels\n",
    "    self.kernel_size = kernel_size\n",
    "    self.dilation_rate = dilation_rate\n",
    "    self.n_blocks = n_blocks\n",
    "    self.n_layers = n_layers\n",
    "    self.p_dropout = p_dropout\n",
    "    self.n_split = n_split\n",
    "    self.n_sqz = n_sqz\n",
    "    self.sigmoid_scale = sigmoid_scale\n",
    "    self.gin_channels = gin_channels\n",
    "\n",
    "    self.flows = nn.ModuleList()\n",
    "    for b in range(n_blocks):\n",
    "      self.flows.append(ActNorm(channels=in_channels * n_sqz))\n",
    "      self.flows.append(InvConvNear(channels=in_channels * n_sqz, n_split=n_split))\n",
    "      self.flows.append(\n",
    "          CouplingBlock(\n",
    "          in_channels * n_sqz,\n",
    "          hidden_channels,\n",
    "          kernel_size=kernel_size, \n",
    "          dilation_rate=dilation_rate,\n",
    "          n_layers=n_layers,\n",
    "          gin_channels=gin_channels,\n",
    "          p_dropout=p_dropout,\n",
    "          sigmoid_scale=sigmoid_scale))\n",
    "\n",
    "  def forward(self, x, x_mask, g=None, reverse=False):\n",
    "    if not reverse:\n",
    "      flows = self.flows\n",
    "      logdet_tot = 0\n",
    "    else:\n",
    "      flows = reversed(self.flows)\n",
    "      logdet_tot = None\n",
    "\n",
    "    if self.n_sqz > 1:\n",
    "      x, x_mask = squeeze(x, x_mask, self.n_sqz)\n",
    "    for f in flows:\n",
    "      if not reverse:\n",
    "        x, logdet = f(x, x_mask, g=g, reverse=reverse)\n",
    "        logdet_tot += logdet\n",
    "      else:\n",
    "        x, logdet = f(x, x_mask, g=g, reverse=reverse)\n",
    "    if self.n_sqz > 1:\n",
    "      x, x_mask = unsqueeze(x, x_mask, self.n_sqz)\n",
    "    return x, logdet_tot\n",
    "\n",
    "  def store_inverse(self):\n",
    "    for f in self.flows:\n",
    "      f.store_inverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FlowSpecDecoder(in_channels=64, \n",
    "              hidden_channels=256, \n",
    "              kernel_size=3, \n",
    "              dilation_rate=1, \n",
    "              n_blocks=4, \n",
    "              n_layers=4, \n",
    "              p_dropout=0., \n",
    "              n_split=4,\n",
    "              n_sqz=2,\n",
    "              sigmoid_scale=False,\n",
    "              gin_channels=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c39f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ste",
   "language": "python",
   "name": "ste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
