{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66dacdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "###\n",
    "class Conv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.c = out_channels\n",
    "        self.filter_out = nn.Conv1d(in_channels=in_channels, out_channels=out_channels*2, \n",
    "                                    kernel_size=kernel_size, stride=stride, dilation=dilation,\n",
    "                                    padding=padding, bias=False)      \n",
    "        self.filter_out.weight.data.normal_(0, 0.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T)\n",
    "        y = self.filter_out(x)\n",
    "        y = F.tanh(y[:, :self.c]) * torch.sigmoid(y[:, self.c:])\n",
    "        \n",
    "        return y\n",
    "    \n",
    "###\n",
    "class NonLinear1d(nn.Module):\n",
    "    def __init__(self, channels, hidden_channels, out_channels, n_layers):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Conv1d(channels, hidden_channels, kernel_size=1)\n",
    "        self.main_convs = nn.ModuleList([Conv1d(hidden_channels, hidden_channels, kernel_size=3,\\\n",
    "                                                dilation=2**(l+1), padding=2**(l+1)) for l in range(n_layers)])\n",
    "        self.skip_convs = nn.ModuleList([Conv1d(hidden_channels, hidden_channels, kernel_size=1)\\\n",
    "                                         for l in range(n_layers)])\n",
    "        self.out_layer = nn.Sequential(nn.ReLU(),\n",
    "                                       nn.Conv1d(hidden_channels, out_channels, kernel_size=1),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Conv1d(out_channels, out_channels, kernel_size=1))\n",
    "                                       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T)\n",
    "        \n",
    "        x = self.in_layer(x)\n",
    "        \n",
    "        skips = []\n",
    "        for main_conv, skip_conv in zip(self.main_convs, self.skip_convs):\n",
    "            y = main_conv(x)\n",
    "            skip = skip_conv(y)\n",
    "            skips.append(skip)\n",
    "            x = x + skip\n",
    "        \n",
    "        y = self.out_layer(sum(skips))\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae1f6d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 100])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 128, 100)\n",
    "# y = Conv1d(128, 128, kernel_size=3, dilation=1, padding=0)(x)\n",
    "# print(y.shape)\n",
    "\n",
    "y = NonLinear1d(128, 256, 128, 4)(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577bec55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dabe5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5060f62d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e777294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ste",
   "language": "python",
   "name": "ste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
